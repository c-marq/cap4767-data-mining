{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c-marq/cap4767-data-mining/blob/main/solutions/exercises/week04_group_exercise_session2_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N94b2mBnk1y"
      },
      "source": [
        "# Week 4 Group Exercise ‚Äî SOLUTION KEY üîë ‚Äî Churn: Logistic Regression vs Neural Network\n",
        "**CAP4767 Data Mining with Python** | Miami Dade College ‚Äî Kendall Campus\n",
        "\n",
        "**Points:** 10 | **Duration:** ~45 minutes | **Deliverable:** Completed notebook + 2‚Äì3 minute presentation\n",
        "\n",
        "**Objective:** Build and compare a logistic regression model and a Keras neural network on the Telco churn dataset. Present your confusion matrices, ROC curves, and model recommendation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cktc71Cwnk1z"
      },
      "source": [
        "### Group Members & Roles\n",
        "\n",
        "| Role | Name | Responsibility |\n",
        "|------|------|----------------|\n",
        "| üñ•Ô∏è **Lead Coder** | | Drives the notebook |\n",
        "| üìä **Data Interpreter** | | Reads outputs, explains metrics |\n",
        "| üé§ **Presenter** | | Delivers the 2‚Äì3 minute share-out |\n",
        "| ‚úÖ **QA Reviewer** | | Checks outputs against checkpoints |\n",
        "\n",
        "*If 3 members, Lead Coder also handles QA.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ooKaF6Qnk1z"
      },
      "source": [
        "<div style=\"background-color: #D6EAF8; border-left: 5px solid #2E86C1; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1A5276;\">üí° GROUP DISCUSSION (before coding ‚Äî 3 minutes)</strong><br>\n",
        "  A telecom company has a $500/year budget per customer for retention efforts. They can only afford to target 200 customers this quarter, but the churn model flags 350 as high-risk.\n",
        "  <ol>\n",
        "    <li>What happens if they target the wrong 200?</li>\n",
        "    <li>Would you rather have a model with high <strong>precision</strong> (fewer false alarms) or high <strong>recall</strong> (catches more churners)? Why?</li>\n",
        "    <li>Is there a scenario where the \"worse\" model on paper is the better business choice?</li>\n",
        "  </ol>\n",
        "</div>\n",
        "\n",
        "**Our group's answers (minimum 3 sentences):**\n",
        "\n",
        "**Sample:** If they target the wrong 200, they waste $100K in retention budget on loyal customers while 150 actual churners leave uncontacted. A retention team should prefer high recall ‚Äî it's better to contact some loyal customers unnecessarily than to miss churners who represent lost lifetime revenue. A model with lower AUC but higher recall on the churned class could be the better business choice if the cost of a missed churner ($500+ lifetime value) far exceeds the cost of a wasted retention call ($50).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArLZDHQnnk1z"
      },
      "source": [
        "<div style=\"background-color: #D5F5E3; border-left: 5px solid #27AE60; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1E8449;\">‚úÖ DO THIS</strong><br>\n",
        "  Run the setup cell below. It loads the dataset, runs the full preprocessing pipeline from the demo, and creates the train/test split. <strong>Do not modify.</strong>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yq2m0jnDnk10"
      },
      "source": [
        "# ============================================================\n",
        "# Setup ‚Äî Run this cell. Do not modify.\n",
        "# Full preprocessing pipeline from the demo.\n",
        "# ============================================================\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (classification_report, confusion_matrix,\n",
        "                             ConfusionMatrixDisplay, roc_curve, roc_auc_score,\n",
        "                             accuracy_score, precision_score, recall_score, f1_score)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
        "plt.rcParams[\"figure.dpi\"] = 100\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Load + preprocess\n",
        "url = \"https://raw.githubusercontent.com/c-marq/cap4767-data-mining/refs/heads/main/data/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"TotalCharges\"]).drop(columns=[\"customerID\"])\n",
        "\n",
        "replace_cols = [\"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\",\n",
        "                \"TechSupport\", \"StreamingTV\", \"StreamingMovies\", \"MultipleLines\"]\n",
        "for col in replace_cols:\n",
        "    df[col] = df[col].replace({\"No internet service\": \"No\", \"No phone service\": \"No\"})\n",
        "\n",
        "binary_cols = [\"Partner\", \"Dependents\", \"PhoneService\", \"PaperlessBilling\", \"Churn\"]\n",
        "for col in binary_cols:\n",
        "    df[col] = df[col].map({\"Yes\": 1, \"No\": 0})\n",
        "df[\"gender\"] = df[\"gender\"].map({\"Male\": 1, \"Female\": 0})\n",
        "for col in replace_cols:\n",
        "    df[col] = df[col].map({\"Yes\": 1, \"No\": 0})\n",
        "\n",
        "df = pd.get_dummies(df, columns=[\"InternetService\", \"Contract\", \"PaymentMethod\"],\n",
        "                     drop_first=True, dtype=int)\n",
        "\n",
        "X = df.drop(columns=[\"Churn\"])\n",
        "y = df[\"Churn\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "continuous = [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"]\n",
        "X_train[continuous] = scaler.fit_transform(X_train[continuous])\n",
        "X_test[continuous] = scaler.transform(X_test[continuous])\n",
        "\n",
        "feature_names = X_train.columns.tolist()\n",
        "n_features = len(feature_names)\n",
        "\n",
        "print(f\"‚úÖ Preprocessing complete\")\n",
        "print(f\"   Features: {n_features} | Train: {X_train.shape[0]:,} | Test: {X_test.shape[0]:,}\")\n",
        "print(f\"   Churn rate ‚Äî Train: {y_train.mean():.1%} | Test: {y_test.mean():.1%}\")\n",
        "\n",
        "# Note: Results may vary slightly across runs even with seeds set.\n",
        "print(f\"   TensorFlow: {tf.__version__}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ny_yXj5nk10"
      },
      "source": [
        "---\n",
        "## Task 1 ‚Äî Build the Logistic Regression Model (1 pt)\n",
        "\n",
        "<div style=\"background-color: #D5F5E3; border-left: 5px solid #27AE60; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1E8449;\">‚úÖ DO THIS</strong><br>\n",
        "  Build and train a <code>LogisticRegression</code> on the scaled training data. Use <code>max_iter=1000, random_state=42</code>.<br>\n",
        "  Store predictions in <code>lr_predictions</code> and probabilities in <code>lr_probabilities</code>.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYAmooSPnk11"
      },
      "source": [
        "# Task 1: Logistic Regression\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_model.fit(X_train, y_train)\n",
        "lr_predictions = lr_model.predict(X_test)\n",
        "lr_probabilities = lr_model.predict_proba(X_test)[:, 1]\n",
        "print(f\"LR Accuracy: {accuracy_score(y_test, lr_predictions):.4f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbOzK-3Fnk11"
      },
      "source": [
        "---\n",
        "## Task 2 ‚Äî Classification Report + Interpretation (1 pt)\n",
        "\n",
        "<div style=\"background-color: #D5F5E3; border-left: 5px solid #27AE60; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1E8449;\">‚úÖ DO THIS</strong><br>\n",
        "  Print the classification report using <code>target_names=['Stayed', 'Churned']</code>.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m03HrEZmnk11"
      },
      "source": [
        "# Task 2: Classification report\n",
        "print(classification_report(y_test, lr_predictions, target_names=[\"Stayed\", \"Churned\"]))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fScBgPknk11"
      },
      "source": [
        "**Interpretation (2‚Äì3 sentences):** What do precision and recall mean *specifically for the \"Churned\" class*? Which metric matters more for a retention team, and why?\n",
        "\n",
        "**Sample:** Precision for Churned means 'of all customers we flagged as likely to churn, what percentage actually did?' ‚Äî it measures how trustworthy our alerts are. Recall for Churned means 'of all customers who actually churned, what percentage did we catch?' ‚Äî it measures how many churners slip through. For a retention team, recall matters more because a missed churner (false negative) costs the company a customer's lifetime value, while a false alarm only costs a retention call."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNYnnPxtnk11"
      },
      "source": [
        "<div style=\"background-color: #FADBD8; border-left: 5px solid #E74C3C; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #922B21;\">üõë CHECKPOINT 1</strong><br>\n",
        "  LR should show ‚âà80% accuracy and 50‚Äì55% recall on Churned. If recall is below 40% or above 70%, check preprocessing.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LBeUPHfnk11"
      },
      "source": [
        "---\n",
        "## Task 3 ‚Äî Build the Keras Neural Network (2 pts)\n",
        "\n",
        "<div style=\"background-color: #D5F5E3; border-left: 5px solid #27AE60; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1E8449;\">‚úÖ DO THIS</strong><br>\n",
        "  Build a Keras Sequential model:\n",
        "  <ul>\n",
        "    <li>Hidden layer 1: <code>n_features</code> neurons, ReLU</li>\n",
        "    <li>Dropout: 0.3</li>\n",
        "    <li>Hidden layer 2: 15 neurons, ReLU</li>\n",
        "    <li>Dropout: 0.2</li>\n",
        "    <li>Output: 1 neuron, sigmoid</li>\n",
        "  </ul>\n",
        "  Compile with Adam + binary crossentropy. Print <code>model.summary()</code>.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-kprbxGnk11"
      },
      "source": [
        "# Task 3: Build ANN\n",
        "model = Sequential([\n",
        "    Dense(n_features, activation=\"relu\", input_shape=(n_features,)),\n",
        "    Dropout(0.3),\n",
        "    Dense(15, activation=\"relu\"),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LtIP7V0nk12"
      },
      "source": [
        "---\n",
        "## Task 4 ‚Äî Train with Early Stopping (1 pt)\n",
        "\n",
        "<div style=\"background-color: #D5F5E3; border-left: 5px solid #27AE60; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1E8449;\">‚úÖ DO THIS</strong><br>\n",
        "  Train with <code>epochs=200, batch_size=32, validation_split=0.2</code>.<br>\n",
        "  Use <code>EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)</code>.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZmnLPCVnk12"
      },
      "source": [
        "# Task 4: Train with early stopping\n",
        "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True, verbose=1)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=200, batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=0\n",
        ")\n",
        "print(f\"Training stopped at epoch {len(history.history['loss'])}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiH3DTS0nk12"
      },
      "source": [
        "---\n",
        "## Task 5 ‚Äî Plot Training Curves (1 pt)\n",
        "\n",
        "<div style=\"background-color: #D5F5E3; border-left: 5px solid #27AE60; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1E8449;\">‚úÖ DO THIS</strong><br>\n",
        "  Side-by-side plot (1 row, 2 cols): training vs validation loss (left) and accuracy (right).\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crkqvaocnk12"
      },
      "source": [
        "# Task 5: Training curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].plot(history.history[\"loss\"], label=\"Training Loss\", color=\"steelblue\")\n",
        "axes[0].plot(history.history[\"val_loss\"], label=\"Validation Loss\", color=\"salmon\")\n",
        "axes[0].set_title(\"Loss Curves\")\n",
        "axes[0].set_xlabel(\"Epoch\")\n",
        "axes[0].set_ylabel(\"Loss\")\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(history.history[\"accuracy\"], label=\"Training Accuracy\", color=\"steelblue\")\n",
        "axes[1].plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\", color=\"salmon\")\n",
        "axes[1].set_title(\"Accuracy Curves\")\n",
        "axes[1].set_xlabel(\"Epoch\")\n",
        "axes[1].set_ylabel(\"Accuracy\")\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwrQ7KuEnk12"
      },
      "source": [
        "**Interpretation (2‚Äì3 sentences):** Is there evidence of overfitting? How can you tell from the curves?\n",
        "\n",
        "**Sample:** The training and validation loss curves track fairly close together, with only a small gap ‚Äî this suggests dropout and early stopping are effectively preventing overfitting. If the validation loss were rising while training loss continued falling, that would indicate overfitting. The early stopping triggered well before 200 epochs, confirming the model found its optimal point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czgBqMRInk12"
      },
      "source": [
        "<div style=\"background-color: #FADBD8; border-left: 5px solid #E74C3C; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #922B21;\">üõë CHECKPOINT 2</strong><br>\n",
        "  Training should stop between epochs 30‚Äì60. Validation loss should track close to training loss. If it ran all 200 epochs, check EarlyStopping config.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j72zBuYsnk12"
      },
      "source": [
        "---\n",
        "## Task 6 ‚Äî Evaluate the ANN (1 pt)\n",
        "\n",
        "<div style=\"background-color: #D5F5E3; border-left: 5px solid #27AE60; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1E8449;\">‚úÖ DO THIS</strong><br>\n",
        "  Generate predictions (threshold=0.5) and probabilities. Print classification report.<br>\n",
        "  Store in <code>ann_predictions</code> and <code>ann_probabilities</code>.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJhW4ckynk12"
      },
      "source": [
        "# Task 6: Evaluate ANN\n",
        "ann_probabilities = model.predict(X_test, verbose=0).ravel()\n",
        "ann_predictions = (ann_probabilities > 0.5).astype(int)\n",
        "\n",
        "print(f\"ANN Accuracy: {accuracy_score(y_test, ann_predictions):.4f}\")\n",
        "print()\n",
        "print(classification_report(y_test, ann_predictions, target_names=[\"Stayed\", \"Churned\"]))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd-BXK3Lnk12"
      },
      "source": [
        "---\n",
        "## Task 7 ‚Äî ROC Curve Comparison (2 pts)\n",
        "\n",
        "<div style=\"background-color: #D5F5E3; border-left: 5px solid #27AE60; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1E8449;\">‚úÖ DO THIS</strong><br>\n",
        "  Plot both ROC curves on a single figure. LR = navy <code>#0f3460</code>, ANN = coral <code>#e94560</code>. Show AUC in legend.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJXssj-rnk13"
      },
      "source": [
        "# Task 7: ROC curve comparison\n",
        "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probabilities)\n",
        "ann_fpr, ann_tpr, _ = roc_curve(y_test, ann_probabilities)\n",
        "\n",
        "lr_auc = roc_auc_score(y_test, lr_probabilities)\n",
        "ann_auc = roc_auc_score(y_test, ann_probabilities)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(lr_fpr, lr_tpr, color=\"#0f3460\", linewidth=2, label=f\"Logistic Regression (AUC={lr_auc:.3f})\")\n",
        "plt.plot(ann_fpr, ann_tpr, color=\"#e94560\", linewidth=2, label=f\"Neural Network (AUC={ann_auc:.3f})\")\n",
        "plt.plot([0, 1], [0, 1], \"k--\", alpha=0.5, label=\"Random Guess\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve Comparison ‚Äî LR vs ANN\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aomnxFDnk13"
      },
      "source": [
        "<div style=\"background-color: #FADBD8; border-left: 5px solid #E74C3C; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #922B21;\">üõë CHECKPOINT 3</strong><br>\n",
        "  ANN should show slightly better recall and AUC than LR (1‚Äì5 percentage points). If dramatically better or worse, check architecture.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSK4ioBXnk13"
      },
      "source": [
        "---\n",
        "## Task 8 ‚Äî Model Recommendation (1 pt)\n",
        "\n",
        "**Answer all of the following (minimum 4 sentences):**\n",
        "\n",
        "1. Which model has better recall on churners?\n",
        "2. Which model has better AUC?\n",
        "3. Which model can explain *why* a customer is flagged?\n",
        "4. If the company can only pick one model, which one and why?\n",
        "5. Is there a scenario where deploying both makes sense?\n",
        "\n",
        "**Sample:** The ANN shows slightly higher recall on churners (catching 2-5% more actual churners), and its AUC is marginally better, meaning it ranks customers by risk more effectively across all thresholds. However, logistic regression can explain *why* a customer is flagged ‚Äî month-to-month contract, fiber optic service, low tenure ‚Äî which the ANN cannot. If the company can only pick one, we recommend logistic regression for the initial deployment because the retention team needs to know *what to say* when they call a customer, not just *who to call*. A strong use case for both: use the ANN to generate the target list, then use LR coefficients to script the retention conversation for each customer segment.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7v7T0Jank13"
      },
      "source": [
        "## Troubleshooting\n",
        "\n",
        "| Problem | Fix |\n",
        "|---------|-----|\n",
        "| ANN accuracy = 0.734 and doesn't change | Model is predicting all \"Stayed\" ‚Äî check architecture and compilation |\n",
        "| `ValueError: shapes not aligned` | Check that `input_shape=(n_features,)` matches your data |\n",
        "| Training runs all 200 epochs | EarlyStopping not in `callbacks` list ‚Äî check `model.fit(callbacks=[early_stop])` |\n",
        "| ROC curve is a straight diagonal | You're plotting predictions (0/1) instead of probabilities ‚Äî use `predict_proba` or `model.predict` |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbOXjf6Snk13"
      },
      "source": [
        "---\n",
        "<p style=\"color:#7F8C8D; font-size:0.85em;\">\n",
        "<em>CAP4767 Data Mining with Python | Miami Dade College | Spring 2026</em><br>\n",
        "Week 4 Group Exercise ‚Äî Churn: LR vs ANN | 10 Points\n",
        "</p>"
      ]
    }
  ]
}