{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c-marq/cap4767-data-mining/blob/main/solutions/labs/lab03_churn_neural_networks_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zE5LV-yp3Dr"
      },
      "source": [
        "# Lab 3 ‚Äî SOLUTION KEY üîë\n",
        "## Churn Prediction: Full Pipeline\n",
        "**CAP4767 Data Mining with Python** | Miami Dade College ‚Äî Kendall Campus\n",
        "\n",
        "**Points:** 20 | **Format:** Individual | **Due:** End of Week 4\n",
        "\n",
        "| Part | Skills (Chapter) | Points |\n",
        "|------|-----------------|--------|\n",
        "| A: EDA | Cram√©r's V, Mann-Whitney U, business cost (Ch. 4) | 4 |\n",
        "| B: Logistic Regression | Baseline model + coefficient interpretation (Ch. 4) | 3 |\n",
        "| C: Neural Network | Keras ANN + dropout + early stopping (Ch. 5) | 4 |\n",
        "| D: Model Comparison | ROC curves + metrics table (Ch. 5) | 3 |\n",
        "| E: Written Analysis | Business recommendation (300+ words) | 4 |\n",
        "| F: Preprocessing | Pipeline runs correctly | 2 |\n",
        "| Bonus | Third model variant | +3 |\n",
        "\n",
        "<div style=\"background-color: #D6EAF8; border-left: 5px solid #2E86C1; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1A5276;\">üí° GRADING PHILOSOPHY</strong><br>\n",
        "  This lab rewards <strong>process over perfection</strong>. If your ANN performs <em>worse</em> than logistic regression, that's a valid result ‚Äî your written analysis should explain why.\n",
        "</div>\n",
        "\n",
        "<div style=\"background-color: #FEF9E7; border-left: 5px solid #F1C40F; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #7D6608;\">‚ö†Ô∏è IMPORTANT</strong><br>\n",
        "  Do NOT use the Telco dataset from class. You must use one of the two options below. Using the Telco dataset = <strong>-5 point deduction</strong>.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pwy_-G2Ap3Dr"
      },
      "source": [
        "### Student Information\n",
        "- **Name:** SOLUTION KEY\n",
        "- **Date:** Spring 2026\n",
        "- **Dataset Chosen:** A (Bank Churn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhqmPhrOp3Ds"
      },
      "source": [
        "---\n",
        "## Setup\n",
        "\n",
        "<div style=\"background-color: #D5F5E3; border-left: 5px solid #27AE60; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1E8449;\">‚úÖ DO THIS</strong><br>\n",
        "  Run this cell. Do not modify.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5-7uodjp3Ds"
      },
      "source": [
        "# ============================================================\n",
        "# Setup ‚Äî Run this cell. Do not modify.\n",
        "# ============================================================\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from scipy.stats import chi2_contingency, mannwhitneyu\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (classification_report, confusion_matrix,\n",
        "                             ConfusionMatrixDisplay, roc_curve, roc_auc_score,\n",
        "                             accuracy_score, precision_score, recall_score, f1_score)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
        "plt.rcParams[\"figure.dpi\"] = 100\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Helper functions (pre-built ‚Äî use these in your EDA)\n",
        "def cramers_v(x, y):\n",
        "    \"\"\"Cram√©r's V: association between two categorical variables (0‚Äì1).\"\"\"\n",
        "    ct = pd.crosstab(x, y)\n",
        "    chi2 = chi2_contingency(ct)[0]\n",
        "    n = ct.sum().sum()\n",
        "    r, k = ct.shape\n",
        "    return np.sqrt(chi2 / (n * (min(r, k) - 1)))\n",
        "\n",
        "def cohens_d(group1, group2):\n",
        "    \"\"\"Cohen's d: effect size between two groups.\"\"\"\n",
        "    n1, n2 = len(group1), len(group2)\n",
        "    pooled = np.sqrt(((n1-1)*group1.std()**2 + (n2-1)*group2.std()**2) / (n1+n2-2))\n",
        "    return (group1.mean() - group2.mean()) / pooled if pooled > 0 else 0\n",
        "\n",
        "print(f\"TensorFlow: {tf.__version__}\")\n",
        "print(\"‚úÖ Setup complete ‚Äî helper functions loaded: cramers_v(), cohens_d()\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGGMuEk8p3Ds"
      },
      "source": [
        "---\n",
        "## Choose Your Dataset + Run Preprocessing\n",
        "\n",
        "<div style=\"background-color: #D5F5E3; border-left: 5px solid #27AE60; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1E8449;\">‚úÖ DO THIS</strong><br>\n",
        "  Uncomment <strong>ONE</strong> option below and run the cell. This handles all preprocessing and gives you clean train/test splits.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFwOmgShp3Dt"
      },
      "source": [
        "# ============================================================\n",
        "# OPTION A ‚Äî Bank Customer Churn (~10,000 rows)\n",
        "# Uncomment the lines below if choosing Option A\n",
        "# ============================================================\n",
        "url = \"https://raw.githubusercontent.com/c-marq/cap4767-data-mining/refs/heads/main/data/Churn_Modelling.csv\"\n",
        "df_raw = pd.read_csv(url)\n",
        "TARGET = \"Exited\"\n",
        "DOMAIN = \"Banking\"\n",
        "\n",
        "# Preprocessing\n",
        "df = df_raw.drop(columns=[\"RowNumber\", \"CustomerId\", \"Surname\"])\n",
        "df[\"Gender\"] = df[\"Gender\"].map({\"Male\": 1, \"Female\": 0})\n",
        "df = pd.get_dummies(df, columns=[\"Geography\"], drop_first=True, dtype=int)\n",
        "\n",
        "# Feature lists for EDA\n",
        "cat_features = [\"Gender\", \"HasCrCard\", \"IsActiveMember\", \"NumOfProducts\",\n",
        "                \"Geography_Germany\", \"Geography_Spain\"]\n",
        "num_features = [\"CreditScore\", \"Age\", \"Tenure\", \"Balance\", \"EstimatedSalary\"]\n",
        "\n",
        "# ============================================================\n",
        "# OPTION B ‚Äî Credit Card Customer Attrition (~10,000 rows)\n",
        "# Uncomment the lines below if choosing Option B\n",
        "# ============================================================\n",
        "# url = \"https://raw.githubusercontent.com/c-marq/cap4767-data-mining/refs/heads/main/data/BankChurners.csv\"\n",
        "# df_raw = pd.read_csv(url)\n",
        "# TARGET = \"Attrition_Flag\"\n",
        "# DOMAIN = \"Credit Card Services\"\n",
        "#\n",
        "# # Preprocessing\n",
        "# # Drop ID and the two Naive Bayes leakage columns\n",
        "# leak_cols = [c for c in df_raw.columns if c.startswith(\"Naive_Bayes\")]\n",
        "# df = df_raw.drop(columns=[\"CLIENTNUM\"] + leak_cols)\n",
        "#\n",
        "# # Encode target: Attrited Customer = 1, Existing Customer = 0\n",
        "# df[TARGET] = df[TARGET].map({\"Attrited Customer\": 1, \"Existing Customer\": 0})\n",
        "#\n",
        "# # Encode categoricals\n",
        "# df[\"Gender\"] = df[\"Gender\"].map({\"M\": 1, \"F\": 0})\n",
        "# df = pd.get_dummies(df, columns=[\"Education_Level\", \"Marital_Status\",\n",
        "#                                    \"Income_Category\", \"Card_Category\"],\n",
        "#                      drop_first=True, dtype=int)\n",
        "#\n",
        "# # Feature lists for EDA\n",
        "# cat_features = [\"Gender\"] + [c for c in df.columns if any(\n",
        "#     c.startswith(p) for p in [\"Education_Level_\", \"Marital_Status_\",\n",
        "#                                \"Income_Category_\", \"Card_Category_\"])]\n",
        "# num_features = [\"Customer_Age\", \"Dependent_count\", \"Months_on_book\",\n",
        "#                 \"Total_Relationship_Count\", \"Months_Inactive_12_mon\",\n",
        "#                 \"Contacts_Count_12_mon\", \"Credit_Limit\", \"Total_Revolving_Bal\",\n",
        "#                 \"Avg_Open_To_Buy\", \"Total_Amt_Chng_Q4_Q1\", \"Total_Trans_Amt\",\n",
        "#                 \"Total_Trans_Ct\", \"Total_Ct_Chng_Q4_Q1\", \"Avg_Utilization_Ratio\"]\n",
        "\n",
        "# ============================================================\n",
        "# Common pipeline (runs for whichever option you chose)\n",
        "# ============================================================\n",
        "X = df.drop(columns=[TARGET])\n",
        "y = df[TARGET]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
        "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
        "\n",
        "feature_names = X_train.columns.tolist()\n",
        "n_features = len(feature_names)\n",
        "\n",
        "print(f\"Dataset: {DOMAIN}\")\n",
        "print(f\"Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns ‚Üí {n_features} features\")\n",
        "print(f\"Train: {X_train.shape[0]:,} | Test: {X_test.shape[0]:,}\")\n",
        "print(f\"Churn rate: {y.mean():.1%}\")\n",
        "print(f\"\\n‚úÖ Preprocessing complete ‚Äî X_train_scaled, X_test_scaled, y_train, y_test ready\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbXKeSjnp3Dt"
      },
      "source": [
        "<div style=\"background-color: #D6EAF8; border-left: 5px solid #2E86C1; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1A5276;\">üí° WHAT THE PREPROCESSING DID</strong><br>\n",
        "  <ul>\n",
        "    <li>Dropped non-predictive ID columns</li>\n",
        "    <li>Encoded the target as binary (1 = churned, 0 = stayed)</li>\n",
        "    <li>Converted categorical features to dummy variables with <code>drop_first=True</code></li>\n",
        "    <li>Scaled all features with <code>StandardScaler</code> (fit on train, transform on test)</li>\n",
        "    <li><strong>Option B only:</strong> Removed two columns that contained pre-computed model outputs ‚Äî using them would be <strong>data leakage</strong> (the model would \"cheat\" by seeing answers derived from the target)</li>\n",
        "  </ul>\n",
        "  <code>cat_features</code> and <code>num_features</code> lists are ready for your EDA.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGKB9vdtp3Dt"
      },
      "source": [
        "---\n",
        "# Part A ‚Äî Exploratory Data Analysis (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCO5b3Qnp3Dt"
      },
      "source": [
        "### Task 1 ‚Äî Data Inspection (1 pt)\n",
        "\n",
        "<div style=\"background-color: #D5F5E3; border-left: 5px solid #27AE60; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1E8449;\">‚úÖ DO THIS</strong><br>\n",
        "  Print the shape, <code>.info()</code>, churn rate, and first 5 rows. Describe the dataset in 2‚Äì3 sentences.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLeBt2sMp3Dt"
      },
      "source": [
        "# Task 1: Data inspection\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"\\nChurn rate: {y.mean():.1%}\")\n",
        "print(f\"\\nChurned: {y.sum():,} | Stayed: {(y==0).sum():,}\")\n",
        "df.info()\n",
        "print()\n",
        "df.head()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z-A2o6Hp3Dt"
      },
      "source": [
        "**Dataset description (2‚Äì3 sentences):**\n",
        "\n",
        "**Sample:** The Bank Customer Churn dataset contains 10,000 customers with 11 features covering demographics (age, gender, geography), banking relationship (tenure, balance, products), and activity status. The churn rate is approximately 20%, meaning roughly 1 in 5 customers left the bank. The dataset is moderately imbalanced ‚Äî the majority class (stayed) is about 4x the minority class (churned)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abZRx2ywp3Du"
      },
      "source": [
        "### Task 2 ‚Äî Cram√©r's V Analysis (1 pt)\n",
        "\n",
        "<div style=\"background-color: #D5F5E3; border-left: 5px solid #27AE60; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1E8449;\">‚úÖ DO THIS</strong><br>\n",
        "  Compute Cram√©r's V between each feature in <code>cat_features</code> and the target. Display as a sorted bar chart.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mFgwHQkp3Du"
      },
      "source": [
        "# Task 2: Cram√©r's V\n",
        "cv_results = pd.DataFrame({\n",
        "    \"Feature\": cat_features,\n",
        "    \"Cram√©r's V\": [cramers_v(df[col], df[TARGET]) for col in cat_features]\n",
        "}).sort_values(\"Cram√©r's V\", ascending=False)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.barh(cv_results[\"Feature\"], cv_results[\"Cram√©r's V\"], color=\"steelblue\")\n",
        "plt.xlabel(\"Cram√©r's V\")\n",
        "plt.title(f\"Categorical Features vs {TARGET} ‚Äî Cram√©r's V\")\n",
        "plt.axvline(x=0.1, color=\"orange\", linestyle=\"--\", alpha=0.7, label=\"Weak (0.1)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(cv_results.to_string(index=False))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0EFGEHSp3Du"
      },
      "source": [
        "**Interpretation (2‚Äì3 sentences):** Which categorical features have the strongest association with churn?\n",
        "\n",
        "**Sample:** IsActiveMember and Geography_Germany show the strongest associations with churn. The Germany geography effect is notable ‚Äî German customers churn at higher rates than French or Spanish customers, possibly reflecting different competitive dynamics or service levels in that market. Gender and HasCrCard show weak associations, suggesting they are not strong churn predictors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDgWYIAep3Du"
      },
      "source": [
        "### Task 3 ‚Äî Mann-Whitney U + Cohen's d (1 pt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EthVdAzUp3Du"
      },
      "source": [
        "# Task 3: Mann-Whitney U + Cohen's d\n",
        "churned = df[df[TARGET] == 1]\n",
        "stayed = df[df[TARGET] == 0]\n",
        "\n",
        "mw_results = []\n",
        "for col in num_features:\n",
        "    u_stat, p_val = mannwhitneyu(churned[col], stayed[col], alternative=\"two-sided\")\n",
        "    d = cohens_d(churned[col], stayed[col])\n",
        "    mw_results.append({\"Feature\": col, \"U Statistic\": f\"{u_stat:,.0f}\",\n",
        "                        \"p-value\": f\"{p_val:.2e}\", \"Cohen's d\": d,\n",
        "                        \"Effect\": \"Large\" if abs(d)>0.8 else \"Medium\" if abs(d)>0.5 else \"Small\"})\n",
        "\n",
        "mw_df = pd.DataFrame(mw_results).sort_values(\"Cohen's d\", key=abs, ascending=False)\n",
        "\n",
        "# Optional: Format as string for display AFTER sorting\n",
        "# mw_df[\"Cohen's d\"] = mw_df[\"Cohen's d\"].map(\"{:.3f}\".format)\n",
        "\n",
        "print(\"Mann-Whitney U + Cohen's d:\")\n",
        "print(mw_df.to_string(index=False))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40pn_xTxp3Du"
      },
      "source": [
        "**Interpretation (2‚Äì3 sentences):** Which numerical features show the largest effect sizes?\n",
        "\n",
        "**Sample:** Age shows the largest Cohen's d ‚Äî churners are significantly older on average, suggesting the bank may be losing its more established customers. Balance also shows a meaningful effect size, with churners having higher balances on average, which is counterintuitive ‚Äî these are valuable customers the bank should be working hardest to retain. Tenure shows a smaller effect than expected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do1BYpcCp3Du"
      },
      "source": [
        "### Task 4 ‚Äî Business Cost Estimate (1 pt)\n",
        "\n",
        "<div style=\"background-color: #D5F5E3; border-left: 5px solid #27AE60; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1E8449;\">‚úÖ DO THIS</strong><br>\n",
        "  Estimate the annual cost of churn. State your assumptions clearly in comments.<br>\n",
        "  Use reasonable estimates for your domain (banking or credit card services).\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6YB9rAYp3Du"
      },
      "source": [
        "# Task 4: Business cost estimate\n",
        "# Assumptions for banking:\n",
        "# - Average annual revenue per customer: ~$1,200 (fees, interest, products)\n",
        "# - Customer acquisition cost: ~$400 (marketing, onboarding)\n",
        "# - Average remaining lifetime: 5 years for retained customers\n",
        "\n",
        "churned_count = y.sum()\n",
        "avg_annual_revenue = 1200   # Conservative banking estimate\n",
        "acquisition_cost = 400      # Industry benchmark\n",
        "remaining_years = 5\n",
        "\n",
        "lifetime_lost = churned_count * avg_annual_revenue * remaining_years\n",
        "replacement = churned_count * acquisition_cost\n",
        "\n",
        "print(f\"Churned customers: {churned_count:,}\")\n",
        "print(f\"Lifetime value lost: ${lifetime_lost:,.0f}\")\n",
        "print(f\"Replacement cost:    ${replacement:,.0f}\")\n",
        "print(f\"TOTAL IMPACT:        ${lifetime_lost + replacement:,.0f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAdqbgfSp3Du"
      },
      "source": [
        "---\n",
        "# Part B ‚Äî Logistic Regression (3 points)\n",
        "\n",
        "### Task 5 ‚Äî Build and Evaluate (1.5 pts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgbW-49pp3Du"
      },
      "source": [
        "# Task 5: Logistic regression\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_model.fit(X_train_scaled, y_train)\n",
        "lr_predictions = lr_model.predict(X_test_scaled)\n",
        "lr_probabilities = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "lr_auc = roc_auc_score(y_test, lr_probabilities)\n",
        "print(\"Logistic Regression:\")\n",
        "print(classification_report(y_test, lr_predictions, target_names=[\"Stayed\", \"Churned\"]))\n",
        "print(f\"AUC: {lr_auc:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AHWuZBIp3Dv"
      },
      "source": [
        "### Task 6 ‚Äî Coefficient Interpretation (1.5 pts)\n",
        "\n",
        "<div style=\"background-color: #D5F5E3; border-left: 5px solid #27AE60; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1E8449;\">‚úÖ DO THIS</strong><br>\n",
        "  Display top 5 positive and top 5 negative coefficients. Explain the top 3 churn drivers in business terms.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giEmWVpkp3Dv"
      },
      "source": [
        "# Task 6: Coefficient interpretation\n",
        "coef_df = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Coefficient\": lr_model.coef_[0]\n",
        "}).sort_values(\"Coefficient\", ascending=False)\n",
        "\n",
        "print(\"Top 5 INCREASING churn risk:\")\n",
        "print(coef_df.head(5).to_string(index=False))\n",
        "print(\"\\nTop 5 DECREASING churn risk:\")\n",
        "print(coef_df.tail(5).to_string(index=False))\n",
        "\n",
        "# Visualization\n",
        "display_df = pd.concat([coef_df.head(5), coef_df.tail(5)])\n",
        "colors = [\"salmon\" if c > 0 else \"steelblue\" for c in display_df[\"Coefficient\"]]\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(display_df[\"Feature\"], display_df[\"Coefficient\"], color=colors)\n",
        "plt.xlabel(\"Coefficient\")\n",
        "plt.title(\"Top Churn Drivers (Logistic Regression)\")\n",
        "plt.axvline(x=0, color=\"black\", linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMhZWB4mp3Dv"
      },
      "source": [
        "**Interpretation (3‚Äì4 sentences):** What does the model say drives churn in this business? Would these findings surprise company leadership?\n",
        "\n",
        "**Sample:** The model reveals that Age and Geography_Germany are among the strongest churn drivers ‚Äî older customers and German-market customers are most at risk. IsActiveMember has a strong negative coefficient, confirming that engaged customers stay. The Balance finding might surprise leadership: customers with higher balances are more likely to churn, suggesting these high-value customers may be getting better offers from competitors. This isn't just a service problem ‚Äî it's a competitive positioning problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8Tdf1hJp3Dv"
      },
      "source": [
        "<div style=\"background-color: #FADBD8; border-left: 5px solid #E74C3C; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #922B21;\">üõë CHECKPOINT</strong><br>\n",
        "  LR should show reasonable accuracy (70‚Äì85%) and an AUC above 0.70. If accuracy equals the majority class rate exactly, the model may be predicting all one class.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQjoDrp_p3Dv"
      },
      "source": [
        "---\n",
        "# Part C ‚Äî Neural Network (4 points)\n",
        "\n",
        "### Task 7 ‚Äî Build and Train a Keras ANN (2 pts)\n",
        "\n",
        "<div style=\"background-color: #D5F5E3; border-left: 5px solid #27AE60; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1E8449;\">‚úÖ DO THIS</strong><br>\n",
        "  Build a Sequential model with at least 2 hidden layers, dropout, and early stopping. Train and capture history.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1owqe_Kp3Dv"
      },
      "source": [
        "# Task 7: Build and train ANN\n",
        "model = Sequential([\n",
        "    Dense(n_features, activation=\"relu\", input_shape=(n_features,)),\n",
        "    Dropout(0.3),\n",
        "    Dense(15, activation=\"relu\"),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10,\n",
        "                            restore_best_weights=True, verbose=1)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=200, batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=0\n",
        ")\n",
        "print(f\"Training stopped at epoch {len(history.history['loss'])}\")\n",
        "model.summary()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXKsj8nJp3Dv"
      },
      "source": [
        "### Task 8 ‚Äî Training Curves (1 pt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nwAAeSGp3Dv"
      },
      "source": [
        "# Task 8: Training curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "epochs_run = len(history.history[\"loss\"])\n",
        "\n",
        "axes[0].plot(history.history[\"loss\"], label=\"Training Loss\", color=\"steelblue\")\n",
        "axes[0].plot(history.history[\"val_loss\"], label=\"Validation Loss\", color=\"salmon\")\n",
        "axes[0].set_title(\"Loss Curves\")\n",
        "axes[0].set_xlabel(\"Epoch\")\n",
        "axes[0].set_ylabel(\"Loss\")\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(history.history[\"accuracy\"], label=\"Training Accuracy\", color=\"steelblue\")\n",
        "axes[1].plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\", color=\"salmon\")\n",
        "axes[1].set_title(\"Accuracy Curves\")\n",
        "axes[1].set_xlabel(\"Epoch\")\n",
        "axes[1].set_ylabel(\"Accuracy\")\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PORjChrEp3Dv"
      },
      "source": [
        "**Interpretation (2‚Äì3 sentences):** What epoch did early stopping trigger? Is there evidence of overfitting?\n",
        "\n",
        "**Sample:** Early stopping triggered around epoch 35-50, well before the maximum 200 epochs. The training and validation loss curves track fairly close together with only a small gap, suggesting that dropout is effectively preventing severe overfitting. The gap is smaller than what we'd see without regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Rh3z0_p3Dw"
      },
      "source": [
        "### Task 9 ‚Äî Evaluate the ANN (1 pt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUnmmTv6p3Dw"
      },
      "source": [
        "# Task 9: Evaluate ANN\n",
        "ann_probabilities = model.predict(X_test_scaled, verbose=0).ravel()\n",
        "ann_predictions = (ann_probabilities > 0.5).astype(int)\n",
        "ann_auc = roc_auc_score(y_test, ann_probabilities)\n",
        "\n",
        "print(\"Neural Network:\")\n",
        "print(classification_report(y_test, ann_predictions, target_names=[\"Stayed\", \"Churned\"]))\n",
        "print(f\"AUC: {ann_auc:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjiON6Bqp3Dw"
      },
      "source": [
        "---\n",
        "# Part D ‚Äî Model Comparison (3 points)\n",
        "\n",
        "### Task 10 ‚Äî ROC + Metrics Table (3 pts)\n",
        "\n",
        "<div style=\"background-color: #D5F5E3; border-left: 5px solid #27AE60; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1E8449;\">‚úÖ DO THIS</strong><br>\n",
        "  <ol>\n",
        "    <li>Plot ROC curves for both models on a single figure (LR = navy, ANN = coral)</li>\n",
        "    <li>Build a comparison table: accuracy, precision, recall, F1, AUC for both</li>\n",
        "    <li>Count customers flagged by ANN but missed by LR</li>\n",
        "  </ol>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pIzdGa7p3Dw"
      },
      "source": [
        "# Task 10: ROC curves + comparison\n",
        "# ROC curves\n",
        "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probabilities)\n",
        "ann_fpr, ann_tpr, _ = roc_curve(y_test, ann_probabilities)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(lr_fpr, lr_tpr, color=\"#0f3460\", linewidth=2, label=f\"Logistic Regression (AUC={lr_auc:.3f})\")\n",
        "plt.plot(ann_fpr, ann_tpr, color=\"#e94560\", linewidth=2, label=f\"Neural Network (AUC={ann_auc:.3f})\")\n",
        "plt.plot([0, 1], [0, 1], \"k--\", alpha=0.5, label=\"Random Guess\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve Comparison\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Comparison table\n",
        "comparison = pd.DataFrame({\n",
        "    \"Metric\": [\"Accuracy\", \"Precision (Churned)\", \"Recall (Churned)\", \"F1 (Churned)\", \"AUC\"],\n",
        "    \"Logistic Regression\": [\n",
        "        f\"{accuracy_score(y_test, lr_predictions):.4f}\",\n",
        "        f\"{precision_score(y_test, lr_predictions):.4f}\",\n",
        "        f\"{recall_score(y_test, lr_predictions):.4f}\",\n",
        "        f\"{f1_score(y_test, lr_predictions):.4f}\",\n",
        "        f\"{lr_auc:.4f}\"\n",
        "    ],\n",
        "    \"Neural Network\": [\n",
        "        f\"{accuracy_score(y_test, ann_predictions):.4f}\",\n",
        "        f\"{precision_score(y_test, ann_predictions):.4f}\",\n",
        "        f\"{recall_score(y_test, ann_predictions):.4f}\",\n",
        "        f\"{f1_score(y_test, ann_predictions):.4f}\",\n",
        "        f\"{ann_auc:.4f}\"\n",
        "    ]\n",
        "})\n",
        "print(comparison.to_string(index=False))\n",
        "\n",
        "# Additional catches\n",
        "ann_only = ((ann_predictions == 1) & (lr_predictions == 0)).sum()\n",
        "print(f\"\\nCustomers flagged by ANN but missed by LR: {ann_only}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt_FVBxjp3Dw"
      },
      "source": [
        "---\n",
        "# Part E ‚Äî Written Analysis (4 points)\n",
        "\n",
        "### Task 11 ‚Äî Model Recommendation (minimum 300 words)\n",
        "\n",
        "Write a recommendation addressed to the business leadership of your chosen domain. Address ALL five points:\n",
        "\n",
        "1. Which model should they deploy for their retention campaign, and why?\n",
        "2. What are the top 3 features driving churn, and what can the business do about each one?\n",
        "3. How many high-risk customers did your models identify? What's the estimated value of retaining them?\n",
        "4. What are the tradeoffs between the two models (accuracy vs interpretability)?\n",
        "5. Is there a scenario where deploying both models makes sense?\n",
        "\n",
        "**To: Senior Leadership, Retail Banking Division**\n",
        "\n",
        "**Recommendation: Deploy logistic regression as the primary model, with the neural network as a secondary screening tool.**\n",
        "\n",
        "After analyzing 10,000 customer records, both models successfully identify customers at risk of leaving the bank. The logistic regression achieves approximately 80% accuracy with an AUC of ~0.77, while the neural network shows a marginal improvement in AUC but sacrifices interpretability. For a banking environment where regulatory compliance and explainability are critical, we recommend logistic regression as the primary deployment.\n",
        "\n",
        "**Top 3 churn drivers and recommended actions:**\n",
        "1. **Age** ‚Äî Older customers churn more, possibly due to changing needs or competitor targeting. Action: develop a premium service tier for customers over 45 with dedicated relationship managers.\n",
        "2. **Geography (Germany)** ‚Äî The German market shows significantly higher churn. Action: conduct a competitive analysis of German banking offerings and consider market-specific retention programs.\n",
        "3. **IsActiveMember status** ‚Äî Inactive customers are far more likely to leave. Action: implement an engagement program that flags customers whose activity drops below baseline and triggers proactive outreach within 30 days.\n",
        "\n",
        "**Risk assessment:** Our models identified approximately 400 high-risk customers in the test set alone. Extrapolating to the full customer base, we estimate 2,000 customers are at elevated risk. At an estimated lifetime value of $6,000 per customer, retaining even 25% of these through targeted intervention represents $3 million in preserved revenue.\n",
        "\n",
        "**Model tradeoffs:** The neural network catches approximately 20-30 additional at-risk customers that logistic regression misses, but it cannot explain *why* they're flagged. In banking, regulators and compliance teams need to understand model decisions. Logistic regression provides that transparency.\n",
        "\n",
        "**Dual deployment scenario:** Use logistic regression to generate the primary target list with clear explanations for each flagged customer. Then run the neural network as a secondary screen to catch additional at-risk customers who didn't score high enough in the primary model. Relationship managers can use the LR coefficients to personalize their retention conversations, while the ANN ensures fewer customers fall through the cracks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3Diahxfp3Dw"
      },
      "source": [
        "---\n",
        "# Bonus Challenge (+3 points)\n",
        "\n",
        "<div style=\"background-color: #D6EAF8; border-left: 5px solid #2E86C1; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1A5276;\">üí° OPTIONAL</strong><br>\n",
        "  Train a <strong>third model</strong> with a meaningfully different architecture. Change at least TWO of: number of layers, neurons per layer, dropout rate, optimizer. Add it to your ROC plot and comparison table.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_MC_UOAp3Dw"
      },
      "source": [
        "# Bonus: Third model ‚Äî wider architecture with SGD optimizer\n",
        "model_v2 = Sequential([\n",
        "    Dense(64, activation=\"relu\", input_shape=(n_features,)),\n",
        "    Dropout(0.4),\n",
        "    Dense(32, activation=\"relu\"),\n",
        "    Dropout(0.3),\n",
        "    Dense(16, activation=\"relu\"),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model_v2.compile(optimizer=\"sgd\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "early_stop_v2 = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\n",
        "history_v2 = model_v2.fit(X_train_scaled, y_train, epochs=200, batch_size=32,\n",
        "                           validation_split=0.2, callbacks=[early_stop_v2], verbose=0)\n",
        "\n",
        "v2_probs = model_v2.predict(X_test_scaled, verbose=0).ravel()\n",
        "v2_preds = (v2_probs > 0.5).astype(int)\n",
        "v2_auc = roc_auc_score(y_test, v2_probs)\n",
        "\n",
        "v2_fpr, v2_tpr, _ = roc_curve(y_test, v2_probs)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(lr_fpr, lr_tpr, color=\"#0f3460\", linewidth=2, label=f\"LR (AUC={lr_auc:.3f})\")\n",
        "plt.plot(ann_fpr, ann_tpr, color=\"#e94560\", linewidth=2, label=f\"ANN v1 (AUC={ann_auc:.3f})\")\n",
        "plt.plot(v2_fpr, v2_tpr, color=\"#2ecc71\", linewidth=2, label=f\"ANN v2 - SGD (AUC={v2_auc:.3f})\")\n",
        "plt.plot([0, 1], [0, 1], \"k--\", alpha=0.5)\n",
        "plt.title(\"Three-Model ROC Comparison\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"ANN v2 Accuracy: {accuracy_score(y_test, v2_preds):.4f}\")\n",
        "print(f\"ANN v2 AUC: {v2_auc:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PuEiQVWp3Dw"
      },
      "source": [
        "**Bonus interpretation (3‚Äì4 sentences):**\n",
        "\n",
        "**Sample:** The SGD-optimized model with a wider architecture (64‚Üí32‚Üí16) and higher dropout rates performed comparably to the Adam-optimized model, though it may have converged more slowly. The difference in AUC between the two ANN variants is likely within random variation, suggesting this dataset doesn't have enough complexity to reward a deeper architecture. This tells us that for this particular churn problem, the bottleneck is the features, not the model ‚Äî more sophisticated architectures can't extract signal that isn't in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HBH2E7Xp3Dx"
      },
      "source": [
        "---\n",
        "## Troubleshooting\n",
        "\n",
        "| Problem | Fix |\n",
        "|---------|-----|\n",
        "| ANN predicts all one class (accuracy = churn rate) | Check architecture ‚Äî may need more neurons or different learning rate |\n",
        "| `ValueError: shapes not aligned` | Verify `input_shape=(n_features,)` matches your feature count |\n",
        "| Option B accuracy is suspiciously high (>95%) | Check that Naive Bayes columns were dropped |\n",
        "| ROC curve is a straight line | Using predictions (0/1) instead of probabilities |\n",
        "| Training runs all 200 epochs | EarlyStopping not in callbacks list |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Oj08HfFp3Dx"
      },
      "source": [
        "---\n",
        "<p style=\"color:#7F8C8D; font-size:0.85em;\">\n",
        "<em>CAP4767 Data Mining with Python | Miami Dade College | Spring 2026</em><br>\n",
        "Lab 3 ‚Äî Churn Prediction: Full Pipeline | 20 Points (+3 Bonus)\n",
        "</p>"
      ]
    }
  ]
}