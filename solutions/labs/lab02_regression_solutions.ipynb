{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c-marq/cap4767-data-mining/blob/main/solutions/labs/lab02_regression_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoZydsyEM5Et"
      },
      "source": [
        "# Lab 2 ‚Äî SOLUTION KEY üîë\n",
        "## Regression Pipeline on a Dataset of Your Choice\n",
        "**CAP4767 Data Mining with Python** | Miami Dade College ‚Äî Kendall Campus\n",
        "\n",
        "**Points:** 20 (+3 bonus) | **Format:** Individual | **Due:** End of Week 3\n",
        "\n",
        "| Part | Skills | Points |\n",
        "|------|--------|--------|\n",
        "| A: EDA | Correlation, scatterplot, data description | 5 |\n",
        "| B: Simple Regression | One predictor, R¬≤, RMSE | 4 |\n",
        "| C: Multiple Regression | Dummies, residuals, coefficients | 5 |\n",
        "| D: Written Interpretation | Model comparison, business application | 6 |\n",
        "| Bonus: Logistic Extension | Binary target, confusion matrix | +3 |\n",
        "\n",
        "<div style=\"background-color: #D6EAF8; border-left: 5px solid #2E86C1; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1A5276;\">üí° GRADING PHILOSOPHY</strong><br>\n",
        "  This lab rewards <strong>process over perfection</strong>. If your code fails but you explain what you tried, you earn most of the points.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHtIJGr-M5Eu"
      },
      "source": [
        "### Student Information\n",
        "- **Name:** SOLUTION KEY\n",
        "- **Date:** Spring 2026\n",
        "- **Dataset Chosen:** California Housing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6k6olacM5Eu"
      },
      "source": [
        "---\n",
        "## Setup\n",
        "\n",
        "<div style=\"background-color: #D5F5E3; border-left: 5px solid #27AE60; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1E8449;\">‚úÖ DO THIS</strong><br>\n",
        "  Run the setup cell, then uncomment your chosen dataset in the next cell.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFiU8wW_M5Eu"
      },
      "source": [
        "# ============================================================\n",
        "# Setup ‚Äî Run this cell. Do not modify.\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (mean_squared_error, r2_score,\n",
        "                             classification_report, confusion_matrix,\n",
        "                             ConfusionMatrixDisplay)\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
        "plt.rcParams[\"figure.dpi\"] = 100\n",
        "sns.set_style(\"whitegrid\")\n",
        "print(\"‚úÖ Setup complete\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZ82G0e1M5Ev"
      },
      "source": [
        "# ============================================================\n",
        "# Choose your dataset ‚Äî uncomment ONE option below\n",
        "# ============================================================\n",
        "\n",
        "# --- Option 1: California Housing ---\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "cal = fetch_california_housing(as_frame=True)\n",
        "df = cal.frame\n",
        "TARGET = \"MedHouseVal\"\n",
        "\n",
        "# --- Option 2: Auto MPG ---\n",
        "# df = sns.load_dataset(\"mpg\").dropna()\n",
        "# TARGET = \"mpg\"\n",
        "\n",
        "# --- Option 3: Medical Insurance ---\n",
        "# insurance_url = \"https://raw.githubusercontent.com/c-marq/cap4767-data-mining/refs/heads/main/data/insurance.csv\"\n",
        "# df = pd.read_csv(insurance_url)\n",
        "# TARGET = \"charges\"\n",
        "\n",
        "print(f\"Dataset: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
        "print(f\"Target: {TARGET}\")\n",
        "df.head()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4ycgFvyM5Ev"
      },
      "source": [
        "---\n",
        "# Part A ‚Äî Exploratory Data Analysis (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPTzS2ggM5Ev"
      },
      "source": [
        "### Task 1: Dataset Description (2 points)\n",
        "\n",
        "<div style=\"background-color: #D5F5E3; border-left: 5px solid #27AE60; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1E8449;\">‚úÖ DO THIS</strong><br>\n",
        "  Display <code>.info()</code> and <code>.describe()</code>. Then write 2‚Äì3 sentences describing the dataset.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItfOx6BCM5Ew"
      },
      "source": [
        "# Task 1: Explore\n",
        "df.info()\n",
        "print()\n",
        "df.describe().round(2)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMcZjhu6M5Ew"
      },
      "source": [
        "**Dataset description (2‚Äì3 sentences):**\n",
        "\n",
        "**Sample:** The California Housing dataset has 20,640 rows, each representing a census block group. It has 8 features including median income, house age, average rooms, and geographic coordinates, with the target being median house value in units of $100K. Each row represents an aggregated neighborhood, not an individual home."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4vlpG2vM5Ew"
      },
      "source": [
        "### Task 2: Correlation Heatmap (1.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsAJ_cbNM5Ew"
      },
      "source": [
        "# Task 2: Sorted correlation with target\n",
        "numeric = df.select_dtypes(include=[np.number])\n",
        "corr = numeric.corr()[TARGET].drop(TARGET).sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "corr.plot(kind=\"barh\", color=[\"steelblue\" if v > 0 else \"salmon\" for v in corr.values])\n",
        "plt.title(f\"Feature Correlations with {TARGET}\")\n",
        "plt.xlabel(\"Pearson Correlation\")\n",
        "plt.axvline(x=0, color=\"black\", linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Top 3:\", corr.head(3).index.tolist())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edgAi4bSM5Ew"
      },
      "source": [
        "### Task 3: Scatterplot of Strongest Predictor (1.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_uXgGaUM5Ew"
      },
      "source": [
        "# Task 3: Scatterplot of strongest predictor\n",
        "best_feature = \"MedInc\"  # Top correlated for California Housing\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(df[best_feature], df[TARGET], alpha=0.1, s=8, color=\"steelblue\")\n",
        "plt.title(f\"{best_feature} vs {TARGET}\")\n",
        "plt.xlabel(best_feature)\n",
        "plt.ylabel(TARGET)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-M0AVB0M5Ew"
      },
      "source": [
        "**What do you see? (1‚Äì2 sentences):**\n",
        "\n",
        "**Sample:** There is a strong positive linear relationship between median income and median house value. The relationship is roughly linear up to about $500K, where the target is capped at $5.0 ($500K), creating a visible ceiling effect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iUWYKgpM5Ew"
      },
      "source": [
        "---\n",
        "# Part B ‚Äî Simple Linear Regression (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1qjFT_YM5Ex"
      },
      "source": [
        "### Task 4: Simple Regression (2 points)\n",
        "\n",
        "<div style=\"background-color: #D5F5E3; border-left: 5px solid #27AE60; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1E8449;\">‚úÖ DO THIS</strong><br>\n",
        "  Build a simple regression using your strongest predictor. Use <code>test_size=0.25, random_state=42</code>. Report R¬≤.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDEEeFzoM5Ex"
      },
      "source": [
        "# Task 4: Simple regression\n",
        "X_simple = df[[\"MedInc\"]]\n",
        "y = df[TARGET]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_simple, y, test_size=0.25, random_state=42)\n",
        "model_simple = LinearRegression().fit(X_train, y_train)\n",
        "r2_simple = model_simple.score(X_test, y_test)\n",
        "y_pred_simple = model_simple.predict(X_test)\n",
        "print(f\"Simple Regression R¬≤: {r2_simple:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_-jfYqpM5Ex"
      },
      "source": [
        "### Task 5: RMSE Interpretation (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh7fCDNtM5Ex"
      },
      "source": [
        "# Task 5: RMSE\n",
        "rmse_simple = np.sqrt(mean_squared_error(y_test, y_pred_simple))\n",
        "print(f\"RMSE: {rmse_simple:.4f} ($100K units)\")\n",
        "print(f\"In dollars: ${rmse_simple * 100_000:,.0f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3TUlz1fM5Ex"
      },
      "source": [
        "**RMSE interpretation (1‚Äì2 sentences):**\n",
        "\n",
        "**Sample:** The RMSE of approximately 0.84 means the model's predictions are off by about $84,000 on average. For a housing market where homes range from $15K to $500K, this is a substantial error ‚Äî the model captures the general trend but misses many neighborhood-level factors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F88Jj8pCM5Ex"
      },
      "source": [
        "<div style=\"background-color: #FADBD8; border-left: 5px solid #E74C3C; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #922B21;\">üõë CHECKPOINT</strong><br>\n",
        "  You should have a baseline R¬≤ and RMSE. If R¬≤ is negative, your feature choice may be poor ‚Äî try a different one.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LK_lnKLM5Ex"
      },
      "source": [
        "---\n",
        "# Part C ‚Äî Multiple Regression (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL2orVR6M5Ex"
      },
      "source": [
        "### Task 6: Multiple Regression with Dummies (2 points)\n",
        "\n",
        "<div style=\"background-color: #D5F5E3; border-left: 5px solid #27AE60; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1E8449;\">‚úÖ DO THIS</strong><br>\n",
        "  Build a multiple regression using 3‚Äì5 features. If categorical columns exist, use <code>pd.get_dummies(drop_first=True)</code>. Report R¬≤ and compare to Task 4.\n",
        "</div>\n",
        "\n",
        "<div style=\"background-color: #FEF9E7; border-left: 5px solid #F1C40F; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #7D6608;\">‚ö†Ô∏è COMMON MISTAKE</strong><br>\n",
        "  If your dataset has string/object columns, sklearn will crash. Use <code>get_dummies()</code> or <code>.select_dtypes(include=[np.number])</code> to keep only numeric features.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9K9e5e2M5Ex"
      },
      "source": [
        "# Task 6: Multiple regression\n",
        "features = [\"MedInc\", \"AveRooms\", \"HouseAge\", \"AveOccup\", \"Latitude\"]\n",
        "X_multi = df[features]\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X_multi, y, test_size=0.25, random_state=42)\n",
        "model_multi = LinearRegression().fit(X_tr, y_tr)\n",
        "r2_multi = model_multi.score(X_te, y_te)\n",
        "y_pred_multi = model_multi.predict(X_te)\n",
        "print(f\"Multiple Regression R¬≤: {r2_multi:.4f} (was {r2_simple:.4f})\")\n",
        "print(f\"Improvement: +{r2_multi - r2_simple:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4EZWSGgM5Ey"
      },
      "source": [
        "### Task 7: Residual Plot (1.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZT9-2NQM5Ey"
      },
      "source": [
        "# Task 7: Residual plot\n",
        "residuals = y_te - y_pred_multi\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(y_pred_multi, residuals, alpha=0.1, s=8, color=\"steelblue\")\n",
        "plt.axhline(y=0, color=\"red\", linewidth=2)\n",
        "plt.title(\"Residual Plot ‚Äî Multiple Regression\")\n",
        "plt.xlabel(\"Predicted Value\")\n",
        "plt.ylabel(\"Residual\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXpfZ10iM5Ey"
      },
      "source": [
        "**Residual interpretation (2+ sentences):**\n",
        "\n",
        "**Sample:** The residuals show a roughly random scatter around zero for lower predicted values, but there is a visible pattern at the upper end ‚Äî the model consistently underpredicts for the most expensive neighborhoods due to the $500K cap in the target variable. There is also a slight funnel shape, with larger errors at higher predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GetlemUcM5Ey"
      },
      "source": [
        "### Task 8: Coefficient Table (1.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlyxiJ0kM5Ey"
      },
      "source": [
        "# Task 8: Coefficients\n",
        "coef_df = pd.DataFrame({\n",
        "    \"Feature\": features,\n",
        "    \"Coefficient\": model_multi.coef_\n",
        "}).sort_values(\"Coefficient\", key=abs, ascending=False)\n",
        "print(coef_df.to_string(index=False))\n",
        "print(f\"\\nIntercept: {model_multi.intercept_:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOf3TLPFM5Ey"
      },
      "source": [
        "---\n",
        "# Part D ‚Äî Written Interpretation (6 points)\n",
        "\n",
        "### Task 9: Analysis (minimum 150 words)\n",
        "\n",
        "Answer ALL four questions:\n",
        "\n",
        "1. **Model comparison:** How much did R¬≤ improve from simple to multiple regression?\n",
        "2. **Feature insight:** Which features were strongest? Any surprises?\n",
        "3. **Business application:** In 2‚Äì3 sentences, explain what your model does for a non-technical audience and give one actionable recommendation.\n",
        "4. **Limitations:** Name one factor NOT in the dataset that would improve predictions.\n",
        "\n",
        "**1. Model comparison:** R¬≤ improved from approximately 0.47 (simple, income only) to approximately 0.60 (multiple, 5 features). The improvement of +0.13 was meaningful ‚Äî adding geographic and housing characteristics gave the model substantially more explanatory power. The added complexity of four more features was justified by the significant accuracy gain.\n",
        "\n",
        "**2. Feature insight:** Median income was by far the strongest predictor, which is expected ‚Äî wealthier neighborhoods have more expensive homes. Latitude was a surprising addition ‚Äî it captures the North/South California price gradient (coastal Southern California is more expensive). Average occupancy had a negative coefficient, meaning crowded neighborhoods tend to have lower property values.\n",
        "\n",
        "**3. Business application:** This model predicts median home values across California neighborhoods based on demographic and housing characteristics. For a real estate investment firm, the key insight is that median income alone explains nearly half of neighborhood pricing, making it the single most important factor to evaluate. Our recommendation: prioritize investment in neighborhoods where median income is rising but home values haven't caught up yet ‚Äî that's where the model predicts the largest undervaluation.\n",
        "\n",
        "**4. Limitations:** School district quality is not in the dataset but is one of the strongest real-world predictors of home values. Adding school ratings, test scores, or proximity to top-rated schools would likely improve R¬≤ by 5‚Äì10 points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBBQBM_GM5Ey"
      },
      "source": [
        "---\n",
        "# Bonus Challenge ‚Äî Logistic Regression Extension (+3 points)\n",
        "\n",
        "<div style=\"background-color: #D6EAF8; border-left: 5px solid #2E86C1; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1A5276;\">üí° OPTIONAL</strong><br>\n",
        "  Create a binary target (e.g., \"above median price\"), build a <code>LogisticRegression</code>, generate a confusion matrix and classification report, and interpret precision/recall in one paragraph. No scaffolding provided.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGxzpaBeM5Ey"
      },
      "source": [
        "# Bonus: Logistic regression extension\n",
        "median_val = df[TARGET].median()\n",
        "df[\"above_median\"] = (df[TARGET] > median_val).astype(int)\n",
        "\n",
        "X_log = df[[\"MedInc\", \"AveRooms\", \"HouseAge\", \"AveOccup\", \"Latitude\"]]\n",
        "y_log = df[\"above_median\"]\n",
        "\n",
        "X_tr_l, X_te_l, y_tr_l, y_te_l = train_test_split(X_log, y_log, test_size=0.25, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_tr_scaled = scaler.fit_transform(X_tr_l)\n",
        "X_te_scaled = scaler.transform(X_te_l)\n",
        "\n",
        "log_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "log_model.fit(X_tr_scaled, y_tr_l)\n",
        "y_pred_log = log_model.predict(X_te_scaled)\n",
        "\n",
        "print(f\"Accuracy: {log_model.score(X_te_scaled, y_te_l):.4f}\")\n",
        "print()\n",
        "print(classification_report(y_te_l, y_pred_log, target_names=[\"Below Median\", \"Above Median\"]))\n",
        "\n",
        "cm = confusion_matrix(y_te_l, y_pred_log)\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=[\"Below Median\", \"Above Median\"])\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "disp.plot(ax=ax, cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix ‚Äî Above/Below Median Price\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYBuGMn3M5Ez"
      },
      "source": [
        "**Bonus interpretation:**\n",
        "\n",
        "**Sample:** The logistic regression achieved approximately 83% accuracy in classifying neighborhoods as above or below median value. Precision for 'above median' was strong (~0.83), meaning when the model predicts a neighborhood is expensive, it's usually right. Recall was similar, meaning it catches most of the truly expensive neighborhoods. The confusion matrix shows the errors are roughly balanced between false positives and false negatives, suggesting no strong bias toward either class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obMfKbfPM5Ez"
      },
      "source": [
        "---\n",
        "## Reflection (required)\n",
        "\n",
        "> *Regression models predict, but they also reveal which features matter. Think about a dataset from your life or career ‚Äî what would you predict, what features would you include, and how would you know if the model was \"good enough\"?*\n",
        "\n",
        "**Your reflection (minimum 3 sentences):**\n",
        "\n",
        "**Sample:** I would predict monthly customer churn at a subscription service using features like usage frequency, support ticket count, subscription tenure, payment method, and plan type. I would know the model was 'good enough' if it could identify at-risk customers at least 2 weeks before they cancel, with precision above 70% ‚Äî meaning most flagged customers would actually churn without intervention. The business value comes not from perfect prediction but from giving the retention team a prioritized list to act on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L12yudPoM5Ez"
      },
      "source": [
        "---\n",
        "## Troubleshooting\n",
        "\n",
        "| Problem | Fix |\n",
        "|---------|-----|\n",
        "| `ValueError: could not convert string` | Categorical columns present ‚Äî use `get_dummies()` or drop them |\n",
        "| R¬≤ is negative | Feature choice is poor ‚Äî try the top-correlated features |\n",
        "| `ValueError: reshape` | Use `df[[\"col\"]]` (double brackets) for X |\n",
        "| `fetch_california_housing` error | Run `!pip install -U scikit-learn` |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcxy8tePM5Ez"
      },
      "source": [
        "---\n",
        "<p style=\"color:#7F8C8D; font-size:0.85em;\">\n",
        "<em>CAP4767 Data Mining with Python | Miami Dade College | Spring 2026</em><br>\n",
        "Lab 2 ‚Äî Regression Pipeline | 20 Points (+3 Bonus)\n",
        "</p>"
      ]
    }
  ]
}