{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c-marq/cap4767-data-mining/blob/main/demos/week03_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqTQz3EnCJRK"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TODO-YOUR-REPO/cap4767-data-mining/blob/main/demos/week03-demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WRXamr5CJRL"
      },
      "source": [
        "# Week 3 Demo ‚Äî Regression: From Linear Relationships to Logistic Classification\n",
        "**CAP4767 Data Mining with Python** | Miami Dade College ‚Äî Kendall Campus\n",
        "\n",
        "**Chapter 3** | Competencies: 1.3, 1.4, 1.5, 1.6, 6 (partial)\n",
        "\n",
        "**What we're building today:**\n",
        "- **Example 1 (Basic):** Simple linear regression ‚Äî one predictor, one target\n",
        "- **Example 2 (Intermediate):** Multiple regression ‚Äî dummy variables, scaling, feature selection\n",
        "- **Example 3 (Full Pipeline):** Complete housing price prediction + logistic regression classifier\n",
        "\n",
        "**Pipeline position:** Regression is the interpretable baseline. Every model you build from here forward ‚Äî decision trees, random forests, neural networks ‚Äî gets compared against a regression baseline first.\n",
        "\n",
        "**Datasets:**\n",
        "| Dataset | Rows | Use |\n",
        "|---------|------|-----|\n",
        "| WA Housing Sales | ~4,600 | Predict home prices (regression) |\n",
        "| Cars | 205 | Feature engineering playground |\n",
        "| UCLA Admissions | 400 | Binary classification (logistic regression) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTO4ls-aCJRM"
      },
      "source": [
        "---\n",
        "## Setup\n",
        "\n",
        "<div style=\"background-color: #D5F5E3; border-left: 5px solid #27AE60; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1E8449;\">‚úÖ DO THIS</strong><br>\n",
        "  Run this cell to load all libraries and datasets. Do not modify.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSDg85skCJRM"
      },
      "source": [
        "# ============================================================\n",
        "# Setup ‚Äî Run this cell. Do not modify.\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
        "from sklearn.metrics import (mean_squared_error, r2_score,\n",
        "                             classification_report, confusion_matrix,\n",
        "                             ConfusionMatrixDisplay)\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
        "plt.rcParams[\"figure.dpi\"] = 100\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Load datasets from GitHub\n",
        "housing_url = \"https://raw.githubusercontent.com/c-marq/cap4767-data-mining/refs/heads/main/data/housingData.csv\"\n",
        "cars_url = \"https://raw.githubusercontent.com/c-marq/cap4767-data-mining/refs/heads/main/data/cars.csv\"\n",
        "admissions_url = \"https://stats.idre.ucla.edu/stat/data/binary.csv\"\n",
        "\n",
        "housing_df = pd.read_csv(housing_url)\n",
        "cars_df = pd.read_csv(cars_url)\n",
        "admissions_df = pd.read_csv(admissions_url)\n",
        "\n",
        "print(f\"Housing:    {housing_df.shape[0]:,} rows √ó {housing_df.shape[1]} columns\")\n",
        "print(f\"Cars:       {cars_df.shape[0]:,} rows √ó {cars_df.shape[1]} columns\")\n",
        "print(f\"Admissions: {admissions_df.shape[0]:,} rows √ó {admissions_df.shape[1]} columns\")\n",
        "print(\"\\n‚úÖ All datasets loaded successfully\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP0j3kWMCJRN"
      },
      "source": [
        "---\n",
        "# Example 1 ‚Äî Simple Linear Regression on Housing Data\n",
        "\n",
        "<div style=\"background-color: #D6EAF8; border-left: 5px solid #2E86C1; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1A5276;\">üí° WHY ARE WE DOING THIS?</strong><br>\n",
        "  Before we throw every feature at a model, we start with the simplest version: <strong>one predictor ‚Üí one target</strong>. This establishes a baseline and teaches the core sklearn workflow: <code>fit()</code> ‚Üí <code>predict()</code> ‚Üí <code>score()</code>. You'll use this exact pattern in every ML model for the rest of the course.\n",
        "</div>\n",
        "\n",
        "**Question:** Can we predict a home's sale price using only its square footage?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqmju0DcCJRN"
      },
      "source": [
        "# 1a. Filter housing data ‚Äî remove outliers\n",
        "housing = housing_df.copy()\n",
        "housing = housing[(housing[\"sqft_living\"] < 8000) &\n",
        "                  (housing[\"price\"] < 1_000_000) &\n",
        "                  (housing[\"price\"] > 0)]\n",
        "\n",
        "print(f\"Filtered: {len(housing):,} rows (removed {len(housing_df) - len(housing):,} outliers)\")\n",
        "print(f\"Price range: ${housing['price'].min():,.0f} ‚Äì ${housing['price'].max():,.0f}\")\n",
        "print(f\"Sqft range:  {housing['sqft_living'].min():,.0f} ‚Äì {housing['sqft_living'].max():,.0f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j081XdKKCJRN"
      },
      "source": [
        "# 1b. EDA ‚Äî scatterplot: does square footage relate to price?\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(housing[\"sqft_living\"], housing[\"price\"], alpha=0.15, s=10, color=\"steelblue\")\n",
        "plt.title(\"Square Footage vs Sale Price ‚Äî WA Housing\")\n",
        "plt.xlabel(\"Living Area (sqft)\")\n",
        "plt.ylabel(\"Sale Price ($)\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv2GxAedCJRN"
      },
      "source": [
        "# 1c. Correlation heatmap ‚Äî which features correlate most with price?\n",
        "numeric_cols = housing.select_dtypes(include=[np.number])\n",
        "corr = numeric_cols.corr()[\"price\"].drop(\"price\").sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "corr.plot(kind=\"barh\", color=[\"steelblue\" if v > 0 else \"salmon\" for v in corr.values])\n",
        "plt.title(\"Feature Correlations with Price (Sorted)\")\n",
        "plt.xlabel(\"Pearson Correlation\")\n",
        "plt.axvline(x=0, color=\"black\", linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Top 5 features correlated with price:\")\n",
        "for feat, val in corr.head(5).items():\n",
        "    print(f\"  {feat:20s} r = {val:.3f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlWzEsY_CJRN"
      },
      "source": [
        "# 1d. Simple linear regression ‚Äî sqft_living ‚Üí price\n",
        "X = housing[[\"sqft_living\"]]     # 2D DataFrame (sklearn requires this)\n",
        "y = housing[\"price\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "model_simple = LinearRegression()\n",
        "model_simple.fit(X_train, y_train)\n",
        "\n",
        "r2_simple = model_simple.score(X_test, y_test)\n",
        "y_pred_simple = model_simple.predict(X_test)\n",
        "rmse_simple = np.sqrt(mean_squared_error(y_test, y_pred_simple))\n",
        "\n",
        "print(f\"Simple Linear Regression (sqft_living only)\")\n",
        "print(f\"  R¬≤:   {r2_simple:.4f}\")\n",
        "print(f\"  RMSE: ${rmse_simple:,.0f}\")\n",
        "print(f\"  Coefficient: ${model_simple.coef_[0]:,.2f} per sqft\")\n",
        "print(f\"  Intercept:   ${model_simple.intercept_:,.2f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BOAC5WECJRN"
      },
      "source": [
        "<div style=\"background-color: #D6EAF8; border-left: 5px solid #2E86C1; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1A5276;\">üí° INTERPRETING R¬≤ = ~0.29</strong><br>\n",
        "  An R¬≤ of 0.29 means square footage explains about 29% of the variation in price. That's a real relationship ‚Äî larger homes do cost more ‚Äî but 71% of the variation comes from other factors: location, condition, year built, and so on. This is our <strong>baseline</strong> to beat.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzylS756CJRO"
      },
      "source": [
        "# 1e. Predicted vs actual scatter\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_pred_simple, alpha=0.15, s=10, color=\"steelblue\")\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],\n",
        "         \"r--\", linewidth=2, label=\"Perfect prediction\")\n",
        "plt.title(f\"Predicted vs Actual ‚Äî Simple Regression (R¬≤={r2_simple:.3f})\")\n",
        "plt.xlabel(\"Actual Price ($)\")\n",
        "plt.ylabel(\"Predicted Price ($)\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jekq5WmdCJRO"
      },
      "source": [
        "# 1f. Residual plot ‚Äî are errors random or systematic?\n",
        "residuals = y_test - y_pred_simple\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(y_pred_simple, residuals, alpha=0.15, s=10, color=\"steelblue\")\n",
        "plt.axhline(y=0, color=\"red\", linewidth=2)\n",
        "plt.title(\"Residual Plot ‚Äî Simple Regression\")\n",
        "plt.xlabel(\"Predicted Price ($)\")\n",
        "plt.ylabel(\"Residual (Actual ‚àí Predicted)\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMTQCtrpCJRO"
      },
      "source": [
        "# 1g. Regression line visualization\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.regplot(x=\"sqft_living\", y=\"price\", data=housing,\n",
        "            scatter_kws={\"alpha\": 0.1, \"s\": 8, \"color\": \"steelblue\"},\n",
        "            line_kws={\"color\": \"red\", \"linewidth\": 2})\n",
        "plt.title(\"Regression Line ‚Äî sqft_living vs price\")\n",
        "plt.xlabel(\"Living Area (sqft)\")\n",
        "plt.ylabel(\"Sale Price ($)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9SFUl6cCJRO"
      },
      "source": [
        "<div style=\"background-color: #FADBD8; border-left: 5px solid #E74C3C; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #922B21;\">üõë STOP AND CHECK ‚Äî Checkpoint 1</strong><br>\n",
        "  <ul>\n",
        "    <li>R¬≤ ‚âà 0.29 ‚Äî one variable captures the trend but leaves most variation unexplained</li>\n",
        "    <li>RMSE ‚âà $130K‚Äì$140K ‚Äî on average, our prediction is off by this much</li>\n",
        "    <li>Residual plot fans out to the right ‚Äî the model is worse at predicting expensive homes</li>\n",
        "    <li>The regression line tilts upward ‚Äî positive relationship confirmed</li>\n",
        "  </ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-0LpYGRCJRO"
      },
      "source": [
        "### ‚ö° Common Error Demo ‚Äî 1D vs 2D Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPP2CpnLCJRO"
      },
      "source": [
        "# ‚ö° DELIBERATE ERROR ‚Äî what happens when you pass a 1D Series?\n",
        "try:\n",
        "    X_wrong = housing[\"sqft_living\"]       # 1D Series ‚Äî WRONG\n",
        "    model_simple.fit(X_wrong, y)\n",
        "except ValueError as e:\n",
        "    print(f\"‚ùå ValueError: {e}\")\n",
        "    print()\n",
        "    print(\"FIX: Use double brackets to create a 2D DataFrame:\")\n",
        "    print('  X = housing[[\"sqft_living\"]]   # ‚Üê 2D DataFrame (correct)')\n",
        "    print('  X = housing[\"sqft_living\"]     # ‚Üê 1D Series (wrong)')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVC9p1rHCJRO"
      },
      "source": [
        "---\n",
        "# Example 2 ‚Äî Multiple Regression + Feature Engineering on Cars Data\n",
        "\n",
        "<div style=\"background-color: #D6EAF8; border-left: 5px solid #2E86C1; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1A5276;\">üí° WHY ARE WE DOING THIS?</strong><br>\n",
        "  Real datasets have categorical variables (text labels), features at different scales, and too many columns. This example teaches three critical skills: (1) converting categories to numbers with <strong>dummy variables</strong>, (2) normalizing features with <strong>StandardScaler</strong>, and (3) automatically picking the best features with <strong>SelectKBest</strong>.\n",
        "</div>\n",
        "\n",
        "**Question:** Can we predict car prices better by adding features, engineering new ones, and selecting wisely?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-rh9HrcCJRO"
      },
      "source": [
        "# 2a. Cars EDA ‚Äî sorted correlation with price\n",
        "cars = cars_df.copy()\n",
        "numeric_cars = cars.select_dtypes(include=[np.number])\n",
        "corr_cars = numeric_cars.corr()[\"price\"].drop(\"price\").sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "corr_cars.plot(kind=\"barh\", color=[\"steelblue\" if v > 0 else \"salmon\" for v in corr_cars.values])\n",
        "plt.title(\"Feature Correlations with Car Price (Sorted)\")\n",
        "plt.xlabel(\"Pearson Correlation\")\n",
        "plt.axvline(x=0, color=\"black\", linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Top 5:\")\n",
        "for feat, val in corr_cars.head(5).items():\n",
        "    print(f\"  {feat:20s} r = {val:.3f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GriGL0OJCJRP"
      },
      "source": [
        "# 2b. Simple regression baseline ‚Äî enginesize only\n",
        "X_cars = cars[[\"enginesize\"]]\n",
        "y_cars = cars[\"price\"]\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X_cars, y_cars, test_size=0.33, random_state=42)\n",
        "\n",
        "model_1var = LinearRegression().fit(X_tr, y_tr)\n",
        "r2_1var = model_1var.score(X_te, y_te)\n",
        "\n",
        "print(f\"Simple Regression (enginesize only)\")\n",
        "print(f\"  R¬≤: {r2_1var:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iezOUGAVCJRP"
      },
      "source": [
        "# 2c. Two-variable model ‚Äî enginesize + curbweight\n",
        "X_cars_2 = cars[[\"enginesize\", \"curbweight\"]]\n",
        "X_tr2, X_te2, y_tr2, y_te2 = train_test_split(X_cars_2, y_cars, test_size=0.33, random_state=42)\n",
        "\n",
        "model_2var = LinearRegression().fit(X_tr2, y_tr2)\n",
        "r2_2var = model_2var.score(X_te2, y_te2)\n",
        "\n",
        "print(f\"Two-Variable Model (enginesize + curbweight)\")\n",
        "print(f\"  R¬≤: {r2_2var:.4f}  (+{r2_2var - r2_1var:.4f} improvement)\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y21R_NtCJRP"
      },
      "source": [
        "# 2d. Identify categorical columns and create dummy variables\n",
        "cat_cols = cars.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "print(f\"Categorical columns ({len(cat_cols)}):\")\n",
        "for col in cat_cols:\n",
        "    print(f\"  {col}: {cars[col].nunique()} unique values ‚Äî {cars[col].unique()[:5].tolist()}...\")\n",
        "\n",
        "# Create dummies for key categoricals\n",
        "cars_encoded = pd.get_dummies(cars, columns=[\"fueltype\", \"aspiration\", \"drivewheel\"],\n",
        "                               drop_first=True, dtype=int)\n",
        "print(f\"\\nAfter dummies: {cars_encoded.shape[1]} columns (was {cars.shape[1]})\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1stuk60CJRP"
      },
      "source": [
        "<div style=\"background-color: #D6EAF8; border-left: 5px solid #2E86C1; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1A5276;\">üí° WHY drop_first=True?</strong><br>\n",
        "  If a car has two fuel types (gas/diesel), one dummy column is enough: <code>fueltype_gas = 1</code> means gas, <code>fueltype_gas = 0</code> means diesel. Creating both columns adds redundant information that confuses the model ‚Äî this is called the <strong>dummy variable trap</strong>.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzpNc8wDCJRP"
      },
      "source": [
        "# 2e. StandardScaler ‚Äî normalize features to the same scale\n",
        "feature_cols = [\"enginesize\", \"curbweight\", \"horsepower\", \"carwidth\", \"citympg\"]\n",
        "X_multi = cars_encoded[feature_cols + [\"fueltype_gas\", \"aspiration_turbo\",\n",
        "                                        \"drivewheel_fwd\", \"drivewheel_rwd\"]]\n",
        "y_multi = cars_encoded[\"price\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = pd.DataFrame(scaler.fit_transform(X_multi), columns=X_multi.columns)\n",
        "\n",
        "print(\"Before scaling (first row):\")\n",
        "print(X_multi.iloc[0].to_string())\n",
        "print(f\"\\nAfter scaling (first row):\")\n",
        "print(X_scaled.iloc[0].round(3).to_string())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSqs3YWFCJRP"
      },
      "source": [
        "# 2f. Multi-feature regression ‚Äî 5 numeric + 4 dummies\n",
        "X_tr_m, X_te_m, y_tr_m, y_te_m = train_test_split(X_scaled, y_multi,\n",
        "                                                     test_size=0.33, random_state=42)\n",
        "\n",
        "model_multi = LinearRegression().fit(X_tr_m, y_tr_m)\n",
        "r2_multi = model_multi.score(X_te_m, y_te_m)\n",
        "r2_train = model_multi.score(X_tr_m, y_tr_m)\n",
        "\n",
        "print(f\"Multi-Feature Model (9 features)\")\n",
        "print(f\"  Train R¬≤: {r2_train:.4f}\")\n",
        "print(f\"  Test R¬≤:  {r2_multi:.4f}\")\n",
        "print(f\"  Gap:      {r2_train - r2_multi:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nOozajICJRP"
      },
      "source": [
        "# 2g. R¬≤ progression ‚Äî showing the value of feature engineering\n",
        "progression = pd.DataFrame({\n",
        "    \"Model\": [\"1 feature (enginesize)\", \"2 features (+curbweight)\", \"9 features (+dummies+scale)\"],\n",
        "    \"Test R¬≤\": [r2_1var, r2_2var, r2_multi]\n",
        "})\n",
        "print(progression.to_string(index=False))\n",
        "\n",
        "# Bar chart\n",
        "plt.figure(figsize=(8, 4))\n",
        "bars = plt.bar(progression[\"Model\"], progression[\"Test R¬≤\"],\n",
        "               color=[\"#AED6F1\", \"#5DADE2\", \"#2E86C1\"])\n",
        "for bar, val in zip(bars, progression[\"Test R¬≤\"]):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "             f\"{val:.3f}\", ha=\"center\", fontsize=11, fontweight=\"bold\")\n",
        "plt.title(\"R¬≤ Progression ‚Äî Adding Features Improves the Model\")\n",
        "plt.ylabel(\"Test R¬≤\")\n",
        "plt.ylim(0, 1)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-qL4L5xCJRQ"
      },
      "source": [
        "# 2h. SelectKBest ‚Äî automated feature ranking\n",
        "all_numeric = cars_encoded.select_dtypes(include=[np.number]).drop(columns=[\"price\"])\n",
        "X_all = all_numeric.fillna(0)\n",
        "y_all = cars_encoded[\"price\"]\n",
        "\n",
        "selector = SelectKBest(score_func=mutual_info_regression, k=10)\n",
        "selector.fit(X_all, y_all)\n",
        "\n",
        "feature_scores = pd.DataFrame({\n",
        "    \"Feature\": X_all.columns,\n",
        "    \"Score\": selector.scores_\n",
        "}).sort_values(\"Score\", ascending=False)\n",
        "\n",
        "print(\"Top 10 Features by Mutual Information:\")\n",
        "print(feature_scores.head(10).to_string(index=False))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JKAnFV4CJRQ"
      },
      "source": [
        "# 2i. Find the \"sweet spot\" ‚Äî train vs test R¬≤ for k=1 to 20\n",
        "results = []\n",
        "for k in range(1, min(21, X_all.shape[1] + 1)):\n",
        "    sel = SelectKBest(score_func=mutual_info_regression, k=k)\n",
        "    X_sel = sel.fit_transform(X_all, y_all)\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X_sel, y_all, test_size=0.33, random_state=42)\n",
        "    m = LinearRegression().fit(Xtr, ytr)\n",
        "    results.append({\"k\": k, \"Train R¬≤\": m.score(Xtr, ytr), \"Test R¬≤\": m.score(Xte, yte)})\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(results_df[\"k\"], results_df[\"Train R¬≤\"], \"o-\", label=\"Train R¬≤\", color=\"steelblue\")\n",
        "plt.plot(results_df[\"k\"], results_df[\"Test R¬≤\"], \"s--\", label=\"Test R¬≤\", color=\"darkorange\")\n",
        "plt.fill_between(results_df[\"k\"], results_df[\"Train R¬≤\"], results_df[\"Test R¬≤\"],\n",
        "                 alpha=0.15, color=\"salmon\", label=\"Overfitting gap\")\n",
        "plt.title(\"Feature Count vs R¬≤ ‚Äî Finding the Sweet Spot\")\n",
        "plt.xlabel(\"Number of Features (k)\")\n",
        "plt.ylabel(\"R¬≤\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "best_k = results_df.loc[results_df[\"Test R¬≤\"].idxmax(), \"k\"]\n",
        "print(f\"\\nüèÜ Best test R¬≤ at k={int(best_k)} features\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIWU8SzgCJRQ"
      },
      "source": [
        "<div style=\"background-color: #FADBD8; border-left: 5px solid #E74C3C; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #922B21;\">üõë STOP AND CHECK ‚Äî Checkpoint 2</strong><br>\n",
        "  <ul>\n",
        "    <li>R¬≤ progressed from ~0.60 ‚Üí ~0.76 ‚Üí ~0.84 ‚Äî each addition improved the model</li>\n",
        "    <li>The sweet spot chart shows the train-test gap widening as features increase ‚Äî that's overfitting</li>\n",
        "    <li>The best test R¬≤ occurs somewhere around k=8‚Äì12 features, not at the maximum</li>\n",
        "  </ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ1Eog-0CJRQ"
      },
      "source": [
        "---\n",
        "# Example 3 ‚Äî Full Housing Pipeline + Logistic Regression Classifier\n",
        "\n",
        "<div style=\"background-color: #D6EAF8; border-left: 5px solid #2E86C1; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1A5276;\">üí° WHY ARE WE DOING THIS?</strong><br>\n",
        "  Two objectives in one example. <strong>First half:</strong> Build a complete housing price model with feature engineering ‚Äî this is the regression pipeline you'll replicate in the lab. <strong>Second half:</strong> Switch to <em>classification</em> with logistic regression ‚Äî when the target is yes/no instead of a dollar amount, the entire evaluation framework changes.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyEh1M3dCJRQ"
      },
      "source": [
        "## Part A ‚Äî Full Housing Regression Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0A4z7q2CJRQ"
      },
      "source": [
        "# 3a. Feature engineering on housing data\n",
        "h = housing.copy()\n",
        "\n",
        "# Create binary feature: has basement?\n",
        "h[\"has_basement\"] = (h[\"sqft_basement\"] > 0).astype(int)\n",
        "\n",
        "# Select features based on correlation analysis from Example 1\n",
        "feature_cols = [\"sqft_living\", \"bathrooms\", \"sqft_above\", \"floors\", \"has_basement\"]\n",
        "X_house = h[feature_cols]\n",
        "y_house = h[\"price\"]\n",
        "\n",
        "print(f\"Features: {feature_cols}\")\n",
        "print(f\"Target: price\")\n",
        "print(f\"Samples: {len(h):,}\")\n",
        "print(f\"\\nhas_basement distribution:\")\n",
        "print(h[\"has_basement\"].value_counts().to_string())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRimkGw-CJRQ"
      },
      "source": [
        "# 3b. Train/test split and fit\n",
        "X_tr_h, X_te_h, y_tr_h, y_te_h = train_test_split(\n",
        "    X_house, y_house, test_size=0.33, random_state=42)\n",
        "\n",
        "model_full = LinearRegression().fit(X_tr_h, y_tr_h)\n",
        "y_pred_full = model_full.predict(X_te_h)\n",
        "\n",
        "r2_full = r2_score(y_te_h, y_pred_full)\n",
        "rmse_full = np.sqrt(mean_squared_error(y_te_h, y_pred_full))\n",
        "\n",
        "print(f\"Full Housing Model (5 features)\")\n",
        "print(f\"  R¬≤:   {r2_full:.4f}\")\n",
        "print(f\"  RMSE: ${rmse_full:,.0f}\")\n",
        "print(f\"\\nCoefficients:\")\n",
        "coef_df = pd.DataFrame({\n",
        "    \"Feature\": feature_cols,\n",
        "    \"Coefficient\": model_full.coef_\n",
        "}).sort_values(\"Coefficient\", ascending=False)\n",
        "print(coef_df.to_string(index=False))\n",
        "print(f\"\\nIntercept: ${model_full.intercept_:,.2f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "931LuSKWCJRQ"
      },
      "source": [
        "# 3c. Residual plot ‚Äî full model\n",
        "residuals_full = y_te_h - y_pred_full\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Predicted vs actual\n",
        "axes[0].scatter(y_te_h, y_pred_full, alpha=0.15, s=10, color=\"steelblue\")\n",
        "axes[0].plot([y_te_h.min(), y_te_h.max()], [y_te_h.min(), y_te_h.max()],\n",
        "             \"r--\", linewidth=2)\n",
        "axes[0].set_title(f\"Predicted vs Actual (R¬≤={r2_full:.3f})\")\n",
        "axes[0].set_xlabel(\"Actual Price ($)\")\n",
        "axes[0].set_ylabel(\"Predicted Price ($)\")\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Residuals\n",
        "axes[1].scatter(y_pred_full, residuals_full, alpha=0.15, s=10, color=\"steelblue\")\n",
        "axes[1].axhline(y=0, color=\"red\", linewidth=2)\n",
        "axes[1].set_title(f\"Residuals (RMSE=${rmse_full:,.0f})\")\n",
        "axes[1].set_xlabel(\"Predicted Price ($)\")\n",
        "axes[1].set_ylabel(\"Residual ($)\")\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVDaUU2MCJRR"
      },
      "source": [
        "<div style=\"background-color: #FADBD8; border-left: 5px solid #E74C3C; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #922B21;\">üõë STOP AND CHECK ‚Äî Checkpoint 3</strong><br>\n",
        "  <ul>\n",
        "    <li>R¬≤ jumped from ~0.29 (simple) to ~0.40+ (multiple) ‚Äî adding features helped</li>\n",
        "    <li>RMSE is now in the $100K‚Äì$120K range ‚Äî still large, but this dataset is limited</li>\n",
        "    <li>Residuals should be roughly randomly scattered around zero</li>\n",
        "  </ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDyxvb_XCJRR"
      },
      "source": [
        "---\n",
        "## Part B ‚Äî Logistic Regression: When the Target is Yes/No\n",
        "\n",
        "<div style=\"background-color: #D6EAF8; border-left: 5px solid #2E86C1; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1A5276;\">üí° WHY ARE WE DOING THIS?</strong><br>\n",
        "  Linear regression predicts <em>continuous</em> values ‚Äî dollars, temperatures, counts. But what if the target is <strong>binary</strong>: admitted/rejected, click/no-click, churn/stay? That's where <strong>logistic regression</strong> comes in. Same <code>.fit()</code> ‚Üí <code>.predict()</code> workflow, completely different evaluation.\n",
        "</div>\n",
        "\n",
        "**Question:** Can we predict graduate school admission using GRE score, GPA, and school prestige?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnmLrgG4CJRR"
      },
      "source": [
        "# 3d. UCLA Admissions ‚Äî EDA\n",
        "print(admissions_df.head())\n",
        "print(f\"\\nAdmission rate: {admissions_df['admit'].mean():.1%}\")\n",
        "print(f\"\\nAdmit rate by prestige rank:\")\n",
        "print(admissions_df.groupby(\"rank\")[\"admit\"].mean().to_string())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIVh1jQFCJRR"
      },
      "source": [
        "# 3e. Visualize: GPA distribution by admission outcome\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# GPA\n",
        "for outcome, color, label in [(1, \"steelblue\", \"Admitted\"), (0, \"salmon\", \"Rejected\")]:\n",
        "    subset = admissions_df[admissions_df[\"admit\"] == outcome]\n",
        "    axes[0].hist(subset[\"gpa\"], bins=15, alpha=0.6, color=color, label=label)\n",
        "axes[0].set_title(\"GPA Distribution by Outcome\")\n",
        "axes[0].set_xlabel(\"GPA\")\n",
        "axes[0].legend()\n",
        "\n",
        "# GRE\n",
        "for outcome, color, label in [(1, \"steelblue\", \"Admitted\"), (0, \"salmon\", \"Rejected\")]:\n",
        "    subset = admissions_df[admissions_df[\"admit\"] == outcome]\n",
        "    axes[1].hist(subset[\"gre\"], bins=15, alpha=0.6, color=color, label=label)\n",
        "axes[1].set_title(\"GRE Distribution by Outcome\")\n",
        "axes[1].set_xlabel(\"GRE Score\")\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ol0K5BLCJRR"
      },
      "source": [
        "# 3f. Prepare features ‚Äî create dummy variables for prestige rank\n",
        "adm = admissions_df.copy()\n",
        "adm = pd.get_dummies(adm, columns=[\"rank\"], drop_first=True, dtype=int)\n",
        "\n",
        "print(\"Features after dummies:\")\n",
        "print(adm.columns.tolist())\n",
        "print(f\"\\nShape: {adm.shape}\")\n",
        "adm.head()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uuq45iJ7CJRR"
      },
      "source": [
        "# 3g. Logistic Regression ‚Äî fit and predict\n",
        "X_adm = adm.drop(columns=[\"admit\"])\n",
        "y_adm = adm[\"admit\"]\n",
        "\n",
        "X_tr_a, X_te_a, y_tr_a, y_te_a = train_test_split(\n",
        "    X_adm, y_adm, test_size=0.25, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler_adm = StandardScaler()\n",
        "X_tr_a_scaled = scaler_adm.fit_transform(X_tr_a)\n",
        "X_te_a_scaled = scaler_adm.transform(X_te_a)\n",
        "\n",
        "log_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "log_model.fit(X_tr_a_scaled, y_tr_a)\n",
        "\n",
        "y_pred_adm = log_model.predict(X_te_a_scaled)\n",
        "accuracy = log_model.score(X_te_a_scaled, y_te_a)\n",
        "\n",
        "print(f\"Logistic Regression ‚Äî Graduate Admissions\")\n",
        "print(f\"  Accuracy: {accuracy:.4f} ({accuracy:.1%})\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dx9_WCzqCJRb"
      },
      "source": [
        "# 3h. Confusion Matrix ‚Äî who did we get right and wrong?\n",
        "cm = confusion_matrix(y_te_a, y_pred_adm)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                               display_labels=[\"Rejected\", \"Admitted\"])\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "disp.plot(ax=ax, cmap=\"Blues\", values_format=\"d\")\n",
        "plt.title(\"Confusion Matrix ‚Äî Graduate Admissions\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Interpret the matrix\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(f\"True Negatives (correctly predicted rejected):  {tn}\")\n",
        "print(f\"False Positives (predicted admit, actually rejected): {fp}\")\n",
        "print(f\"False Negatives (predicted reject, actually admitted): {fn}\")\n",
        "print(f\"True Positives (correctly predicted admitted):  {tp}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cKHsJNyCJRb"
      },
      "source": [
        "# 3i. Classification Report ‚Äî precision, recall, F1\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_te_a, y_pred_adm,\n",
        "                            target_names=[\"Rejected\", \"Admitted\"]))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMQ0tNcsCJRb"
      },
      "source": [
        "<div style=\"background-color: #D6EAF8; border-left: 5px solid #2E86C1; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1A5276;\">üí° READING THE CLASSIFICATION REPORT</strong><br>\n",
        "  <ul>\n",
        "    <li><strong>Precision</strong> = \"Of everyone we <em>predicted</em> would be admitted, what % actually were?\" ‚Äî answers \"How trustworthy are our positive predictions?\"</li>\n",
        "    <li><strong>Recall</strong> = \"Of everyone who was <em>actually</em> admitted, what % did we catch?\" ‚Äî answers \"How many real admits did we miss?\"</li>\n",
        "    <li><strong>F1</strong> = The harmonic mean of precision and recall ‚Äî a single number that balances both</li>\n",
        "    <li><strong>Support</strong> = The number of actual cases in each class</li>\n",
        "  </ul>\n",
        "  For an admissions office: high <em>recall</em> means you're not accidentally rejecting strong candidates. High <em>precision</em> means you're not wasting interview slots on unlikely admits.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUTUfKHRCJRb"
      },
      "source": [
        "<div style=\"background-color: #FADBD8; border-left: 5px solid #E74C3C; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #922B21;\">üõë STOP AND CHECK ‚Äî Checkpoint 4</strong><br>\n",
        "  <ul>\n",
        "    <li>Accuracy ‚âà 0.70‚Äì0.75 ‚Äî decent but not spectacular (the baseline of \"predict everyone rejected\" would be ~0.68)</li>\n",
        "    <li>The confusion matrix shows the model is better at predicting rejections than admissions</li>\n",
        "    <li>Recall for \"Admitted\" is likely lower than for \"Rejected\" ‚Äî the model is conservative</li>\n",
        "  </ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wjJfvIwCJRb"
      },
      "source": [
        "---\n",
        "## Regression vs Classification ‚Äî When to Use Which\n",
        "\n",
        "| Question | Linear Regression | Logistic Regression |\n",
        "|----------|------------------|-------------------|\n",
        "| Target type | Continuous (dollars, sqft, degrees) | Binary (yes/no, 0/1) |\n",
        "| Output | A number on a scale | A probability (0‚Äì1) ‚Üí class label |\n",
        "| Evaluation | R¬≤, RMSE, residual plot | Accuracy, precision, recall, F1, confusion matrix |\n",
        "| sklearn class | `LinearRegression()` | `LogisticRegression()` |\n",
        "| Same workflow? | ‚úÖ `.fit()` ‚Üí `.predict()` ‚Üí `.score()` | ‚úÖ `.fit()` ‚Üí `.predict()` ‚Üí `.score()` |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdJHYsvxCJRc"
      },
      "source": [
        "---\n",
        "## Takeaway\n",
        "\n",
        "<div style=\"background-color: #D5F5E3; border-left: 5px solid #27AE60; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1E8449;\">‚úÖ WHAT WE BUILT TODAY</strong><br>\n",
        "  Three types of regression models ‚Äî simple, multiple, and logistic ‚Äî using the same sklearn <code>.fit()</code> ‚Üí <code>.predict()</code> ‚Üí <code>.score()</code> workflow. The only things that change are the features, the model class, and the evaluation metrics.\n",
        "</div>\n",
        "\n",
        "**Pipeline position:** Regression is the interpretable baseline. Every model you build from here forward will be compared against a regression baseline.\n",
        "\n",
        "**Next chapter preview:** In Chapter 4, you'll apply logistic regression to a real business problem: predicting which of 7,032 telecom customers are about to cancel their service ‚Äî and putting a $509,000 price tag on the cost of getting it wrong."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtmaN5EmCJRc"
      },
      "source": [
        "---\n",
        "<p style=\"color:#7F8C8D; font-size:0.85em;\">\n",
        "<em>CAP4767 Data Mining with Python | Miami Dade College | Spring 2026</em><br>\n",
        "Week 3 Demo ‚Äî Regression: Simple, Multiple, and Logistic | 19 code cells across 3 examples\n",
        "</p>"
      ]
    }
  ]
}