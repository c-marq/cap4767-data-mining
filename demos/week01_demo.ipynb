{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c-marq/cap4767-data-mining/blob/main/demos/week01_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQg99tfgz05A"
      },
      "source": [
        "# Week 1 Demo ‚Äî Time Series Foundations\n",
        "**CAP4767 Data Mining with Python** | Miami Dade College ‚Äî Kendall Campus\n",
        "\n",
        "---\n",
        "\n",
        "**What we're building today:** The core pandas toolkit for working with time-indexed data ‚Äî generating date ranges, reindexing, resampling, rolling windows, and running totals.\n",
        "\n",
        "**Why this matters for data mining:** Nearly every real-world dataset has a time dimension. Stock prices, customer transactions, sensor readings, wildfire records ‚Äî the patterns hidden in *when* things happen are often more valuable than the events themselves. This week gives you the foundation that every forecasting model in Weeks 2‚Äì7 will depend on.\n",
        "\n",
        "**Datasets:**\n",
        "- `AAPL.csv` ‚Äî Apple stock price data (daily OHLC, 2020)\n",
        "- `acresBurned.csv` ‚Äî California wildfire acres burned by discovery date (1992‚Äì2015)\n",
        "\n",
        "**Adapted from:** Murach's *Python for Data Science*, Chapter 9"
      ],
      "id": "CQg99tfgz05A"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaoMgr6Bz05B"
      },
      "source": [
        "<div style=\"background-color: #D6EAF8; border-left: 5px solid #2E86C1; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "<strong style=\"color: #1A5276;\">üí° WHY ARE WE DOING THIS?</strong><br>\n",
        "<strong>Where does this fit in the data mining pipeline?</strong><br><br>Time series analysis is the <em>first stage</em> of our forecasting pipeline. Before we can predict anything (Week 2: SARIMAX &amp; Prophet), we need to know how to manipulate time-indexed data ‚Äî resample it, smooth it, and reshape it. Think of today's skills like learning to prep ingredients before you cook. Every forecasting model downstream expects clean, properly indexed time series as input.\n",
        "</div>"
      ],
      "id": "xaoMgr6Bz05B"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWjz72E5z05C"
      },
      "source": [
        "<div style=\"background-color: #D5F5E3; border-left: 5px solid #27AE60; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "<strong style=\"color: #1E8449;\">‚úÖ DO THIS</strong><br>\n",
        "Run the next two cells to load our libraries and datasets. <strong>Do not modify these cells.</strong>\n",
        "</div>"
      ],
      "id": "VWjz72E5z05C"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfpjleYSz05C"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Setup ‚Äî Run this cell. Do not modify.\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime as dt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ Libraries loaded successfully.\")"
      ],
      "id": "MfpjleYSz05C"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sRMe4P7z05D"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Load datasets ‚Äî Run this cell. Do not modify.\n",
        "# ============================================================\n",
        "# TODO: Replace these URLs with your GitHub raw URLs after pushing to repo\n",
        "aapl_url = \"https://raw.githubusercontent.com/c-marq/cap4767-data-mining/refs/heads/main/data/stocks.csv\"\n",
        "acres_url = \"https://raw.githubusercontent.com/c-marq/cap4767-data-mining/refs/heads/main/data/acresBurned.csv\"\n",
        "\n",
        "# Apple stock data ‚Äî daily OHLC prices for 2020\n",
        "stockData = pd.read_csv(aapl_url,\n",
        "                        usecols=['Date', 'Open', 'High', 'Low', 'Close'],\n",
        "                        parse_dates=['Date'])\n",
        "stockData.set_index('Date', inplace=True)\n",
        "\n",
        "# California wildfire acres burned by discovery date\n",
        "# NOTE: The CSV must include the 'discovery_date' column as the first column.\n",
        "# If your CSV only has 'acres_burned', re-export from the .pkl with index=True.\n",
        "acresBurned = pd.read_csv(acres_url,\n",
        "                          index_col='discovery_date',\n",
        "                          parse_dates=True)\n",
        "\n",
        "print(f\"‚úÖ stockData loaded: {stockData.shape[0]} rows, {stockData.shape[1]} columns\")\n",
        "print(f\"‚úÖ acresBurned loaded: {acresBurned.shape[0]} rows, {acresBurned.shape[1]} columns\")"
      ],
      "id": "9sRMe4P7z05D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZx0-dvaz05D"
      },
      "source": [
        "---\n",
        "## Section 1: How to Generate Time Periods"
      ],
      "id": "bZx0-dvaz05D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENSLpXtMz05E"
      },
      "source": [
        "<div style=\"background-color: #D6EAF8; border-left: 5px solid #2E86C1; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "<strong style=\"color: #1A5276;\">üí° WHY ARE WE DOING THIS?</strong><br>\n",
        "Before we can analyze time-series data, we need to understand how pandas represents time. The <code>pd.date_range()</code> function is your go-to tool for generating sequences of dates. Think of it like a ruler for time ‚Äî you set the start, the end, and the spacing between tick marks. This becomes critical when we need to reindex stock data to specific intervals (like every Friday) or resample wildfire data to monthly totals.\n",
        "</div>"
      ],
      "id": "ENSLpXtMz05E"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtqk0UOwz05E"
      },
      "source": [
        "### Monthly Start Dates (`freq='MS'`)\n",
        "Generate the first day of every month in 2020:"
      ],
      "id": "Wtqk0UOwz05E"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WY52z389z05E"
      },
      "outputs": [],
      "source": [
        "# MS = Month Start ‚Äî gives us Jan 1, Feb 1, Mar 1, etc.\n",
        "pd.date_range('01/01/2020', '12/31/2020', freq='MS')"
      ],
      "id": "WY52z389z05E"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5q9GMTwz05E"
      },
      "source": [
        "### Business Days (`freq='B'`)\n",
        "Generate only weekday dates ‚Äî no Saturdays or Sundays:"
      ],
      "id": "s5q9GMTwz05E"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB0iiaoKz05F"
      },
      "outputs": [],
      "source": [
        "# B = Business days ‚Äî skips weekends automatically\n",
        "pd.date_range('01/01/2020', '01/31/2020', freq='B')"
      ],
      "id": "pB0iiaoKz05F"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZNfElzMz05F"
      },
      "source": [
        "### Weekly on Mondays (`freq='W-MON'`)\n",
        "Generate every Monday in December 2020:"
      ],
      "id": "XZNfElzMz05F"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_hNZqCxz05F"
      },
      "outputs": [],
      "source": [
        "# W-MON = Weekly anchored on Monday\n",
        "pd.date_range('12/01/2020', '12/31/2020', freq='W-MON')"
      ],
      "id": "u_hNZqCxz05F"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE57I2gPz05F"
      },
      "source": [
        "### Sub-Daily: 12-Hour Intervals (`freq='12h'`)\n",
        "Time periods aren't limited to days ‚Äî we can go as granular as hours, minutes, or seconds:"
      ],
      "id": "DE57I2gPz05F"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgS8axFPz05F"
      },
      "outputs": [],
      "source": [
        "# 12h = every 12 hours ‚Äî useful for shift-based or sensor data\n",
        "pd.date_range('01/01/2020', '01/31/2020', freq='12h')"
      ],
      "id": "zgS8axFPz05F"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M21HBOEz05F"
      },
      "source": [
        "<div style=\"background-color: #FEF9E7; border-left: 5px solid #F1C40F; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "<strong style=\"color: #7D6608;\">‚ö†Ô∏è COMMON MISTAKE</strong><br>\n",
        "<strong>Pandas version note:</strong> The Murach textbook uses uppercase frequency codes like <code>'H'</code>, <code>'M'</code>, and <code>'Q'</code>. In pandas 2.2+ (which Google Colab now uses), these have been replaced:<br>\n",
        "‚Ä¢ <code>'H'</code> ‚Üí <code>'h'</code> (hours) ¬∑ <code>'M'</code> ‚Üí <code>'ME'</code> (month-end) ¬∑ <code>'Q'</code> ‚Üí <code>'QE'</code> (quarter-end)<br><br>\n",
        "If you see an <code>Invalid frequency</code> error, switch to the updated code shown in this notebook. Codes like <code>'MS'</code>, <code>'B'</code>, <code>'W-FRI'</code>, and <code>'SMS'</code> are unchanged.\n",
        "</div>"
      ],
      "id": "2M21HBOEz05F"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waRubmolz05F"
      },
      "source": [
        "<div style=\"background-color: #FADBD8; border-left: 5px solid #E74C3C; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "<strong style=\"color: #922B21;\">üõë STOP AND CHECK</strong><br>\n",
        "<strong>Checkpoint ‚Äî Section 1</strong><br><br>You should see four different <code>DatetimeIndex</code> outputs above:<br>‚Ä¢ 12 monthly dates (Jan‚ÄìDec 2020)<br>‚Ä¢ 23 business days in January 2020<br>‚Ä¢ 4 Mondays in December 2020<br>‚Ä¢ 61 twelve-hour intervals in January 2020<br><br>If any cell produced an error, check that the Setup cell ran successfully first.\n",
        "</div>"
      ],
      "id": "waRubmolz05F"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvpzqvTBz05F"
      },
      "source": [
        "---\n",
        "## Section 2: Reindexing with Datetime Indexes"
      ],
      "id": "YvpzqvTBz05F"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNxt4_Pbz05G"
      },
      "source": [
        "<div style=\"background-color: #D6EAF8; border-left: 5px solid #2E86C1; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "<strong style=\"color: #1A5276;\">üí° WHY ARE WE DOING THIS?</strong><br>\n",
        "Real-world data doesn't always come on the schedule we need. Stock markets are closed on weekends and holidays, but your analysis might need data at regular intervals ‚Äî every Friday, every two weeks, or every month. <code>reindex()</code> lets us reshape a DataFrame to match a new set of dates. Dates with no matching data get <code>NaN</code> ‚Äî and those NaN values are actually useful information: they tell us <em>when data is missing</em>.\n",
        "</div>"
      ],
      "id": "tNxt4_Pbz05G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZdrpOHrz05G"
      },
      "source": [
        "### Quick look at our stock data"
      ],
      "id": "JZdrpOHrz05G"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nq3LZbKhz05G"
      },
      "outputs": [],
      "source": [
        "# AAPL daily stock data ‚Äî the index is already a DatetimeIndex\n",
        "stockData.head(3)"
      ],
      "id": "Nq3LZbKhz05G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUKy2t76z05G"
      },
      "source": [
        "### Reindex to Fridays only\n",
        "Let's extract only Friday closing prices ‚Äî a common view for weekly investment reporting:"
      ],
      "id": "SUKy2t76z05G"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVtIDVC_z05G"
      },
      "outputs": [],
      "source": [
        "# Generate every Friday in 2020\n",
        "fridays = pd.date_range('01/01/2020', '12/31/2020', freq='W-FRI')\n",
        "print(f\"Generated {len(fridays)} Fridays in 2020\")\n",
        "\n",
        "# Reindex: keep only rows that fall on a Friday\n",
        "stockData.reindex(fridays).head(3)"
      ],
      "id": "cVtIDVC_z05G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsOvNTDPz05G"
      },
      "source": [
        "<div style=\"background-color: #FEF9E7; border-left: 5px solid #F1C40F; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "<strong style=\"color: #7D6608;\">‚ö†Ô∏è COMMON MISTAKE</strong><br>\n",
        "Notice: some Fridays may show <code>NaN</code>. This happens when the stock market was closed on that Friday (like Good Friday or the day after Thanksgiving). <code>reindex()</code> doesn't invent data ‚Äî if there's no trading record for that date, you get NaN. That's the correct behavior.\n",
        "</div>"
      ],
      "id": "ZsOvNTDPz05G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83yTGW82z05G"
      },
      "source": [
        "<div style=\"background-color: #FADBD8; border-left: 5px solid #E74C3C; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "<strong style=\"color: #922B21;\">üõë STOP AND CHECK</strong><br>\n",
        "<strong>Checkpoint ‚Äî Section 2</strong><br><br>Your Friday reindex should show 52 Fridays and display 3 rows of AAPL data. The first Friday (2020-01-03) should show Open ‚âà 74.29 and Close ‚âà 74.36.\n",
        "</div>"
      ],
      "id": "83yTGW82z05G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZxoBss3z05G"
      },
      "source": [
        "---\n",
        "## Section 3: Semi-Month Reindexing & Fixing Weekend Dates"
      ],
      "id": "zZxoBss3z05G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hz7R33tLz05G"
      },
      "source": [
        "<div style=\"background-color: #D6EAF8; border-left: 5px solid #2E86C1; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "<strong style=\"color: #1A5276;\">üí° WHY ARE WE DOING THIS?</strong><br>\n",
        "Some financial reports use <strong>semi-monthly</strong> intervals ‚Äî the 1st and 15th of each month. But what happens when the 1st or 15th lands on a weekend? The stock market is closed, so there's no data. We'll first see the problem, then build a custom function to fix it. This is a great example of why data wrangling is never just \"load and go\" ‚Äî real data always has edge cases.\n",
        "</div>"
      ],
      "id": "Hz7R33tLz05G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMJurWrvz05H"
      },
      "source": [
        "### The Problem: Semi-Month Dates on Weekends"
      ],
      "id": "dMJurWrvz05H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyroXrGfz05H"
      },
      "outputs": [],
      "source": [
        "# SMS = Semi-Month Start (1st and 15th of each month)\n",
        "semiMonths = pd.date_range('01/01/2020', '12/31/2020', freq='SMS')\n",
        "print(f\"Generated {len(semiMonths)} semi-month dates\")\n",
        "semiMonths[:6]  # Show the first few"
      ],
      "id": "DyroXrGfz05H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JADsI8wYz05H"
      },
      "outputs": [],
      "source": [
        "# Reindex stock data to semi-month dates ‚Äî notice the NaN values!\n",
        "stockData.reindex(semiMonths).head()"
      ],
      "id": "JADsI8wYz05H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyat32juz05H"
      },
      "outputs": [],
      "source": [
        "# Plot it ‚Äî the gaps from NaN values create ugly breaks in the line\n",
        "stockData.reindex(semiMonths).plot(title='Semi-Month Reindex ‚Äî Before Fix (Notice the Gaps)')\n",
        "plt.tight_layout()"
      ],
      "id": "iyat32juz05H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSUI1awxz05H"
      },
      "source": [
        "<div style=\"background-color: #FEF9E7; border-left: 5px solid #F1C40F; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "<strong style=\"color: #7D6608;\">‚ö†Ô∏è COMMON MISTAKE</strong><br>\n",
        "See the NaN rows? January 1st was a holiday (New Year's Day), February 1st was a Saturday, February 15th was a Saturday ‚Äî none of these had trading data. The plot has visible gaps. We need a smarter approach.\n",
        "</div>"
      ],
      "id": "rSUI1awxz05H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9KYE5Rgz05H"
      },
      "source": [
        "### The Fix: A Custom `adjustDate()` Function\n",
        "If a date falls on Saturday, shift it back to Friday. If it falls on Sunday, shift it forward to Monday:"
      ],
      "id": "g9KYE5Rgz05H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qfJJnTCz05H"
      },
      "outputs": [],
      "source": [
        "def adjustDate(date):\n",
        "    \"\"\"Shift weekend dates to the nearest weekday.\n",
        "    Saturday ‚Üí Friday (go back 1 day)\n",
        "    Sunday   ‚Üí Monday (go forward 1 day)\n",
        "    Weekdays ‚Üí no change\n",
        "    \"\"\"\n",
        "    if date.weekday() < 5:       # Mon-Fri (0-4) ‚Äî already a weekday\n",
        "        return date\n",
        "    elif date.weekday() == 5:    # Saturday ‚Äî shift to Friday\n",
        "        return date - dt.timedelta(days=1)\n",
        "    else:                        # Sunday ‚Äî shift to Monday\n",
        "        return date + dt.timedelta(days=1)"
      ],
      "id": "9qfJJnTCz05H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwI5SwOaz05H"
      },
      "outputs": [],
      "source": [
        "# Apply the fix: adjust each semi-month date to the nearest business day\n",
        "semiMonths = pd.date_range('01/01/2020', '12/31/2020', freq='SMS')\n",
        "semiMonthsAdjusted = semiMonths.to_series().apply(adjustDate)\n",
        "\n",
        "# Compare original vs adjusted ‚Äî look at the dates that changed\n",
        "comparison = pd.DataFrame({\n",
        "    'Original': semiMonths[:6],\n",
        "    'Adjusted': semiMonthsAdjusted.values[:6]\n",
        "})\n",
        "comparison['Changed?'] = comparison['Original'] != comparison['Adjusted']\n",
        "comparison"
      ],
      "id": "vwI5SwOaz05H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TRHZuJ4z05I"
      },
      "outputs": [],
      "source": [
        "# Now reindex with the adjusted dates ‚Äî no more NaN!\n",
        "stockData.reindex(semiMonthsAdjusted).head()"
      ],
      "id": "8TRHZuJ4z05I"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEowsjbSz05I"
      },
      "outputs": [],
      "source": [
        "# Plot the fixed version ‚Äî smooth, continuous line\n",
        "stockData.reindex(semiMonthsAdjusted).plot(\n",
        "    title='Semi-Month Reindex ‚Äî After Fix (Clean!)')\n",
        "plt.tight_layout()"
      ],
      "id": "GEowsjbSz05I"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCem7SNLz05I"
      },
      "source": [
        "<div style=\"background-color: #FADBD8; border-left: 5px solid #E74C3C; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "<strong style=\"color: #922B21;\">üõë STOP AND CHECK</strong><br>\n",
        "<strong>Checkpoint ‚Äî Section 3</strong><br><br>Compare the two plots:<br>‚Ä¢ <strong>Before fix:</strong> Broken lines with gaps where NaN values dropped out<br>‚Ä¢ <strong>After fix:</strong> Smooth, continuous lines across all 24 semi-month points<br><br>The adjusted reindex should show <strong>no NaN values</strong> in the first 5 rows. January 1st (holiday) should shift to January 2nd. February 1st (Saturday) should shift to January 31st.\n",
        "</div>"
      ],
      "id": "aCem7SNLz05I"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reKRC1VGz05L"
      },
      "source": [
        "---\n",
        "## Section 4: Resampling Time Series Data"
      ],
      "id": "reKRC1VGz05L"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__s0oegHz05M"
      },
      "source": [
        "<div style=\"background-color: #D6EAF8; border-left: 5px solid #2E86C1; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "<strong style=\"color: #1A5276;\">üí° WHY ARE WE DOING THIS?</strong><br>\n",
        "Reindexing picks specific dates from existing data. <strong>Resampling</strong> is different ‚Äî it <em>aggregates</em> data into new time buckets. Think of it like this:<br><br>‚Ä¢ <strong>Reindexing</strong> = \"Show me only the data from these specific dates\" (like filtering)<br>‚Ä¢ <strong>Resampling</strong> = \"Combine all the data within each month/quarter/year into one number\" (like GROUP BY in SQL)<br><br>We'll use the California wildfire dataset to see how daily acres-burned data can be rolled up to monthly and quarterly totals.\n",
        "</div>"
      ],
      "id": "__s0oegHz05M"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yilynTVlz05M"
      },
      "source": [
        "### Quick look at the wildfire data"
      ],
      "id": "yilynTVlz05M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEaTdaZ8z05M"
      },
      "outputs": [],
      "source": [
        "# California wildfire acres burned ‚Äî daily records from 1992 onward\n",
        "acresBurned.head(3)"
      ],
      "id": "SEaTdaZ8z05M"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmEKRUk0z05M"
      },
      "source": [
        "### Monthly Totals with `resample()`"
      ],
      "id": "lmEKRUk0z05M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpVExlyKz05M"
      },
      "outputs": [],
      "source": [
        "# Resample to monthly frequency, summing all acres burned within each month\n",
        "acresBurned.resample(rule='ME').sum().head(3)"
      ],
      "id": "HpVExlyKz05M"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpZR-GSNz05M"
      },
      "source": [
        "### Upsampling: Going *More* Granular (12-Hour Bins)"
      ],
      "id": "QpZR-GSNz05M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpEecobfz05M"
      },
      "outputs": [],
      "source": [
        "# What happens if we resample to a SMALLER interval than the data has?\n",
        "# 12h = 12-hour bins. Since original data is daily, the new rows get 0.\n",
        "acresBurned.resample(rule='12h').sum().head(4)"
      ],
      "id": "dpEecobfz05M"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5bLOjXUz05M"
      },
      "source": [
        "<div style=\"background-color: #FEF9E7; border-left: 5px solid #F1C40F; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "<strong style=\"color: #7D6608;\">‚ö†Ô∏è COMMON MISTAKE</strong><br>\n",
        "Upsampling (going to a finer resolution) fills new intervals with 0 when using <code>.sum()</code> because there's nothing to aggregate. If you used <code>.mean()</code> instead, you'd get NaN for the new intervals. Choose your aggregation function carefully based on what the zeros or NaNs mean in your domain.\n",
        "</div>"
      ],
      "id": "j5bLOjXUz05M"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVc-XxJPz05M"
      },
      "source": [
        "### Controlling Bin Boundaries: `label` and `closed`\n",
        "When resampling to quarters, should January 1st belong to Q1 or the previous Q4? The `label` and `closed` parameters give you control:"
      ],
      "id": "MVc-XxJPz05M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYLOfaWsz05N"
      },
      "outputs": [],
      "source": [
        "# label='right', closed='right': Q1 ends March 31, labeled as March 31\n",
        "acresBurned.resample(rule='QE', label='right', closed='right').sum().head()"
      ],
      "id": "kYLOfaWsz05N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AI5hqFx3z05N"
      },
      "outputs": [],
      "source": [
        "# label='left', closed='left': Q1 starts Jan 1, labeled as Jan 1\n",
        "acresBurned.resample(rule='QE', label='left', closed='left').sum().head()"
      ],
      "id": "AI5hqFx3z05N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Voqltmlz05N"
      },
      "source": [
        "<div style=\"background-color: #FADBD8; border-left: 5px solid #E74C3C; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "<strong style=\"color: #922B21;\">üõë STOP AND CHECK</strong><br>\n",
        "<strong>Checkpoint ‚Äî Section 4</strong><br><br>Your monthly resample should show aggregated totals per month. The first row (1992-01-31) should be the sum of all acres burned in January 1992.<br><br>The quarterly resample with <code>label='right'</code> should show dates like 1992-03-31, 1992-06-30, etc.<br>The quarterly resample with <code>label='left'</code> should show dates like 1992-01-01, 1992-04-01, etc.\n",
        "</div>"
      ],
      "id": "1Voqltmlz05N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RWJofvPz05N"
      },
      "source": [
        "---\n",
        "## Section 5: How Downsampling Improves Plots"
      ],
      "id": "-RWJofvPz05N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnwJwynLz05N"
      },
      "source": [
        "<div style=\"background-color: #D6EAF8; border-left: 5px solid #2E86C1; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "<strong style=\"color: #1A5276;\">üí° WHY ARE WE DOING THIS?</strong><br>\n",
        "Raw daily stock data can be noisy ‚Äî every small fluctuation shows up in the plot, making it hard to see the overall trend. <strong>Downsampling</strong> (resampling to a coarser frequency) smooths the data. Here we compare the daily Apple closing price to the weekly average. This is the same idea behind the \"zoom out\" feature in any stock charting app.\n",
        "</div>"
      ],
      "id": "TnwJwynLz05N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJXgnyZcz05N"
      },
      "outputs": [],
      "source": [
        "# Daily closing price ‚Äî lots of noise\n",
        "stockData.plot(y='Close', title='AAPL Daily Close Price (2020)', legend=False)\n",
        "plt.ylabel('Price ($)')\n",
        "plt.tight_layout()"
      ],
      "id": "ZJXgnyZcz05N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CooCbWsMz05N"
      },
      "outputs": [],
      "source": [
        "# Weekly average closing price ‚Äî smoother trend line\n",
        "stockData.resample(rule='W').mean().plot(\n",
        "    y='Close', title='AAPL Weekly Mean Close Price (2020)', legend=False)\n",
        "plt.ylabel('Price ($)')\n",
        "plt.tight_layout()"
      ],
      "id": "CooCbWsMz05N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MatzpV6z05N"
      },
      "source": [
        "<div style=\"background-color: #FADBD8; border-left: 5px solid #E74C3C; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "<strong style=\"color: #922B21;\">üõë STOP AND CHECK</strong><br>\n",
        "<strong>Checkpoint ‚Äî Section 5</strong><br><br>Compare the two plots side by side:<br>‚Ä¢ The <strong>daily</strong> plot shows every jitter and gap (‚âà253 data points)<br>‚Ä¢ The <strong>weekly mean</strong> plot is smoother, showing the clear trend: AAPL started ~$75 in January, crashed to ~$60 in March (COVID), then rallied to ~$130 by year end<br><br>Both plots should render without errors.\n",
        "</div>"
      ],
      "id": "8MatzpV6z05N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCUOKMB7z05O"
      },
      "source": [
        "---\n",
        "## Section 6: Rolling Windows"
      ],
      "id": "fCUOKMB7z05O"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SiAOlBsz05O"
      },
      "source": [
        "<div style=\"background-color: #D6EAF8; border-left: 5px solid #2E86C1; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "<strong style=\"color: #1A5276;\">üí° WHY ARE WE DOING THIS?</strong><br>\n",
        "Downsampling compresses data into fewer points. <strong>Rolling windows</strong> take a different approach ‚Äî they keep every data point but replace each value with the average of its surrounding neighbors. Imagine sliding a magnifying glass across the data, where at each position you compute the average of the last N days.<br><br>This is one of the most common smoothing techniques in data mining, used in everything from stock technical analysis to IoT sensor anomaly detection.\n",
        "</div>"
      ],
      "id": "4SiAOlBsz05O"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS0R-3LOz05O"
      },
      "source": [
        "### Raw vs. Smoothed: Concept Visualization\n",
        "Let's look at January 2020 AAPL High/Low prices ‚Äî first raw, then with a 7-day rolling mean:"
      ],
      "id": "IS0R-3LOz05O"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okam1zAiz05O"
      },
      "outputs": [],
      "source": [
        "# Raw High and Low prices for January 2020\n",
        "df_raw = stockData[['High', 'Low']].query('Date <= \"01/31/2020\"')\n",
        "\n",
        "g = sns.relplot(data=df_raw, kind='line', markers=True, aspect=1.5)\n",
        "g.figure.suptitle('AAPL High/Low ‚Äî Raw Daily (Jan 2020)', y=1.02)\n",
        "for ax in g.axes.flat:\n",
        "    ax.tick_params('x', labelrotation=90)\n",
        "    ax.set_xticks(pd.date_range(start='01/02/2020', end='01/31/2020', freq='B'))\n",
        "    ax.set_xticklabels(pd.date_range(start='01/02/2020', end='01/31/2020',\n",
        "                                     freq='B').strftime('%m-%d'))"
      ],
      "id": "okam1zAiz05O"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EypPToRz05O"
      },
      "outputs": [],
      "source": [
        "# Same data with a 7-day rolling mean applied ‚Äî notice how the lines smooth out\n",
        "df_smooth = stockData[['High', 'Low']].query('Date <= \"01/31/2020\"') \\\n",
        "                                      .rolling(window=7, min_periods=7).mean()\n",
        "\n",
        "g = sns.relplot(data=df_smooth, kind='line', markers=True, aspect=1.5)\n",
        "g.figure.suptitle('AAPL High/Low ‚Äî 7-Day Rolling Mean (Jan 2020)', y=1.02)\n",
        "for ax in g.axes.flat:\n",
        "    ax.tick_params('x', labelrotation=90)\n",
        "    ax.set_xticks(pd.date_range(start='01/02/2020', end='01/31/2020', freq='B'))\n",
        "    ax.set_xticklabels(pd.date_range(start='01/02/2020', end='01/31/2020',\n",
        "                                     freq='B').strftime('%m-%d'))"
      ],
      "id": "_EypPToRz05O"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jjYdNX5z05O"
      },
      "source": [
        "### Understanding `min_periods`\n",
        "The `window` parameter sets how many observations to include. But what happens at the start when you don't have 7 days of data yet?"
      ],
      "id": "3jjYdNX5z05O"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwb0OcP6z05O"
      },
      "outputs": [],
      "source": [
        "# Default: min_periods = window size (7)\n",
        "# First 6 rows are NaN because there aren't enough prior points\n",
        "df_strict = stockData[['High', 'Low']].query('Date <= \"01/31/2020\"') \\\n",
        "                                      .rolling(window=7).mean()\n",
        "print(\"With min_periods=7 (default) ‚Äî first 8 rows:\")\n",
        "df_strict.head(8)"
      ],
      "id": "bwb0OcP6z05O"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRFtO9Zxz05O"
      },
      "outputs": [],
      "source": [
        "# Relaxed: min_periods=1 means \"compute the average with whatever you have\"\n",
        "# No NaN values ‚Äî the first row uses just 1 value, second uses 2, etc.\n",
        "df_relaxed = stockData[['High', 'Low']].query('Date <= \"01/31/2020\"') \\\n",
        "                                       .rolling(window=7, min_periods=1).mean()\n",
        "print(\"With min_periods=1 ‚Äî first 8 rows:\")\n",
        "df_relaxed.head(8)"
      ],
      "id": "xRFtO9Zxz05O"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyePXXuyz05P"
      },
      "source": [
        "<div style=\"background-color: #FEF9E7; border-left: 5px solid #F1C40F; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "<strong style=\"color: #7D6608;\">‚ö†Ô∏è COMMON MISTAKE</strong><br>\n",
        "When should you use <code>min_periods=1</code> vs. the default?<br><br>‚Ä¢ Use the <strong>default</strong> (strict) when accuracy matters ‚Äî you want a true 7-day average, not a partial one<br>‚Ä¢ Use <code>min_periods=1</code> when you need a value for every row (e.g., for downstream plotting or modeling) and can tolerate the early values being less stable\n",
        "</div>"
      ],
      "id": "pyePXXuyz05P"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7apF9eCz05P"
      },
      "source": [
        "### Final Rolling Window Plot"
      ],
      "id": "g7apF9eCz05P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRHVpX5lz05P"
      },
      "outputs": [],
      "source": [
        "# Clean rolling window plot with strict min_periods\n",
        "df_plot = stockData[['High', 'Low']].query('Date <= \"01/31/2020\"') \\\n",
        "                                    .rolling(window=7, min_periods=7).mean()\n",
        "\n",
        "g = sns.relplot(data=df_plot, kind='line', markers=True, aspect=1.5)\n",
        "g.figure.suptitle('AAPL 7-Day Rolling Mean ‚Äî High/Low (Jan 2020)', y=1.02)\n",
        "for ax in g.axes.flat:\n",
        "    ax.tick_params('x', labelrotation=90)\n",
        "    ax.set_xticks(pd.date_range(start='01/10/2020', end='01/31/2020', freq='B'))\n",
        "    ax.set_xticklabels(pd.date_range(start='01/10/2020', end='01/31/2020',\n",
        "                                     freq='B').strftime('%Y-%m-%d'))"
      ],
      "id": "xRHVpX5lz05P"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auBpBu_Iz05P"
      },
      "source": [
        "<div style=\"background-color: #FADBD8; border-left: 5px solid #E74C3C; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "<strong style=\"color: #922B21;\">üõë STOP AND CHECK</strong><br>\n",
        "<strong>Checkpoint ‚Äî Section 6</strong><br><br>You should see three plots and two DataFrames:<br>‚Ä¢ <strong>Raw plot:</strong> Jagged High/Low lines with sharp day-to-day swings<br>‚Ä¢ <strong>Smoothed plot:</strong> Gentler curves ‚Äî the 7-day rolling mean absorbs the daily noise<br>‚Ä¢ <strong>Default min_periods:</strong> First 6 rows are NaN (not enough data for a full 7-day window)<br>‚Ä¢ <strong>min_periods=1:</strong> All rows have values, but early values are based on fewer observations<br>‚Ä¢ <strong>Final plot:</strong> Clean rolling mean starting from Jan 10 (first date with a full 7-day window)\n",
        "</div>"
      ],
      "id": "auBpBu_Iz05P"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWX8vnQ_z05P"
      },
      "source": [
        "---\n",
        "## Section 7: Running Totals with `expanding()`"
      ],
      "id": "HWX8vnQ_z05P"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXsGlFgnz05P"
      },
      "source": [
        "<div style=\"background-color: #D6EAF8; border-left: 5px solid #2E86C1; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "<strong style=\"color: #1A5276;\">üí° WHY ARE WE DOING THIS?</strong><br>\n",
        "A <strong>running total</strong> (also called a cumulative sum) keeps a running tally as you move through time. At any point, it tells you the total from the very beginning up to that moment. This is how you'd answer: \"How many total acres have burned <em>so far</em> this year?\"<br><br><code>expanding()</code> is like <code>rolling()</code> but the window always starts at row 1 and grows ‚Äî it never drops old data.\n",
        "</div>"
      ],
      "id": "BXsGlFgnz05P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lo00rbgKz05P"
      },
      "outputs": [],
      "source": [
        "# Reload acresBurned fresh (we'll add a column to it)\n",
        "acresBurned_rt = acresBurned.copy()\n",
        "\n",
        "# expanding().sum() = cumulative sum from the beginning\n",
        "acresBurned_rt['running_total'] = acresBurned_rt['acres_burned'].expanding().sum()\n",
        "acresBurned_rt.head()"
      ],
      "id": "Lo00rbgKz05P"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD46yNaVz05P"
      },
      "source": [
        "### Visualizing Daily Burn vs. Running Total\n",
        "Let's use the first 10 days to see the bar chart clearly. We'll use `pd.melt()` to reshape the data for Seaborn's grouped bar chart:"
      ],
      "id": "YD46yNaVz05P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EChzJ4V3z05Q"
      },
      "outputs": [],
      "source": [
        "# Take the first 10 days for a readable chart\n",
        "acresPlot = acresBurned_rt.head(10).copy()\n",
        "acresPlot.reset_index(inplace=True)\n",
        "acresPlot.head(3)"
      ],
      "id": "EChzJ4V3z05Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vh2KLh5Oz05Q"
      },
      "outputs": [],
      "source": [
        "# Melt from wide to long format ‚Äî one row per date+metric combination\n",
        "acresMelted = pd.melt(acresPlot,\n",
        "                      id_vars='discovery_date',\n",
        "                      value_vars=['acres_burned', 'running_total'],\n",
        "                      var_name='value_type')\n",
        "acresMelted.head(3)"
      ],
      "id": "Vh2KLh5Oz05Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vc601IT1z05Q"
      },
      "outputs": [],
      "source": [
        "# Grouped bar chart: daily burn (blue) vs running total (orange)\n",
        "g = sns.catplot(data=acresMelted, kind='bar',\n",
        "                x='discovery_date', y='value', hue='value_type', aspect=1.5)\n",
        "g.figure.suptitle('Daily Acres Burned vs. Running Total (First 10 Days)', y=1.02)\n",
        "for ax in g.axes.flat:\n",
        "    ax.tick_params('x', labelrotation=90)\n",
        "    ax.set_xticklabels(acresMelted.discovery_date.drop_duplicates().astype(str))"
      ],
      "id": "Vc601IT1z05Q"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKTAxueXz05Q"
      },
      "source": [
        "<div style=\"background-color: #FADBD8; border-left: 5px solid #E74C3C; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "<strong style=\"color: #922B21;\">üõë STOP AND CHECK</strong><br>\n",
        "<strong>Checkpoint ‚Äî Section 7</strong><br><br>Your bar chart should show 10 pairs of bars:<br>‚Ä¢ <strong>acres_burned</strong> (blue) ‚Äî the daily value, varies from day to day<br>‚Ä¢ <strong>running_total</strong> (orange) ‚Äî grows monotonically, never decreases<br><br>By day 10, the running total should be significantly larger than any single day's burn. This pattern ‚Äî small daily increments building into a large cumulative total ‚Äî is the signature of running totals.\n",
        "</div>"
      ],
      "id": "aKTAxueXz05Q"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XqPwVgAz05Q"
      },
      "source": [
        "---\n",
        "## Wrap-Up: What We Learned & What's Next\n",
        "\n",
        "**Today we built the time series toolkit:**\n",
        "\n",
        "| Technique | pandas Method | What It Does |\n",
        "|-----------|--------------|--------------|\n",
        "| Date ranges | `pd.date_range()` | Generate sequences of dates at any frequency |\n",
        "| Reindexing | `df.reindex()` | Reshape data to a new set of dates |\n",
        "| Date adjustment | Custom `adjustDate()` | Fix weekend dates to nearest business day |\n",
        "| Resampling | `df.resample()` | Aggregate data into time buckets (daily ‚Üí monthly) |\n",
        "| Rolling windows | `df.rolling()` | Smooth data with a sliding average |\n",
        "| Running totals | `df.expanding().sum()` | Cumulative sum from the beginning |\n",
        "\n",
        "**Next week (Week 2):** We take these skills and use them to actually *predict the future* ‚Äî time series forecasting with SARIMAX and Prophet. Everything we did today (resampling, smoothing, datetime indexing) is prerequisite input for those models.\n",
        "\n",
        "---\n",
        "*CAP4767 Data Mining with Python | Miami Dade College | Spring 2026*"
      ],
      "id": "4XqPwVgAz05Q"
    }
  ]
}