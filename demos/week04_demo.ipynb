{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c-marq/cap4767-data-mining/blob/main/demos/week04_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43sguuMOlv0O"
      },
      "source": [
        "# Week 4 Demo ‚Äî Customer Churn: EDA ‚Üí Logistic Regression ‚Üí Neural Networks\n",
        "**CAP4767 Data Mining with Python** | Miami Dade College ‚Äî Kendall Campus\n",
        "\n",
        "**Chapters 4 & 5** | Competencies: 1.3, 1.4, 1.5, 1.6, 6 (partial)\n",
        "\n",
        "**What we're building today:**\n",
        "\n",
        "| Session | Content | Chapter |\n",
        "|---------|---------|---------|\n",
        "| **Session 1** | Statistical EDA + Logistic Regression Baseline | Ch. 4 |\n",
        "| **Session 2** | Neural Networks + Model Comparison + Risk Scoring | Ch. 5 |\n",
        "\n",
        "**The business question:** Of 7,032 telecom customers, which ones are about to cancel ‚Äî and what's the dollar cost of getting it wrong?\n",
        "\n",
        "**Pipeline position:** Week 3 taught you regression on continuous targets. This week we shift to **binary classification** (churn/stay) and introduce **neural networks** as an alternative to logistic regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twoxmo4Slv0O"
      },
      "source": [
        "---\n",
        "## Setup\n",
        "\n",
        "<div style=\"background-color: #D5F5E3; border-left: 5px solid #27AE60; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1E8449;\">‚úÖ DO THIS</strong><br>\n",
        "  Run this cell to load all libraries and suppress TensorFlow warnings. Do not modify.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3GNomS3lv0O"
      },
      "source": [
        "# ============================================================\n",
        "# Setup ‚Äî Run this cell. Do not modify.\n",
        "# ============================================================\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from scipy.stats import chi2_contingency, mannwhitneyu, pointbiserialr\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import (classification_report, confusion_matrix,\n",
        "                             ConfusionMatrixDisplay, roc_curve, roc_auc_score,\n",
        "                             accuracy_score, precision_score, recall_score, f1_score)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
        "plt.rcParams[\"figure.dpi\"] = 100\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(\"‚úÖ All libraries loaded\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLaS6DlElv0P"
      },
      "source": [
        "---\n",
        "## Load the Telco Churn Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IohmF_M7lv0P"
      },
      "source": [
        "# Load from GitHub\n",
        "url = \"https://raw.githubusercontent.com/c-marq/cap4767-data-mining/refs/heads/main/data/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "print(f\"Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
        "print(f\"\\nChurn distribution:\")\n",
        "print(df[\"Churn\"].value_counts())\n",
        "print(f\"\\nChurn rate: {df['Churn'].value_counts(normalize=True)['Yes']:.1%}\")\n",
        "df.head()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4NNwW5Olv0P"
      },
      "source": [
        "# Data quality: TotalCharges has blanks (new customers with tenure=0)\n",
        "df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\")\n",
        "print(f\"Blank TotalCharges: {df['TotalCharges'].isna().sum()} rows (tenure=0 new customers)\")\n",
        "df = df.dropna(subset=[\"TotalCharges\"])\n",
        "df = df.drop(columns=[\"customerID\"])\n",
        "print(f\"After cleanup: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y104K0N1lv0Q"
      },
      "source": [
        "---\n",
        "# SESSION 1 ‚Äî Chapter 4: EDA + Logistic Regression\n",
        "\n",
        "---\n",
        "# Example 1 ‚Äî Statistical EDA for Classification\n",
        "\n",
        "<div style=\"background-color: #D6EAF8; border-left: 5px solid #2E86C1; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1A5276;\">üí° WHY ARE WE DOING THIS?</strong><br>\n",
        "  In regression (Week 3), we used correlation to find predictors. With a <strong>binary target</strong> (Yes/No), correlation doesn't work for categorical features. We need different tools: <strong>Cram√©r's V</strong> for categorical features and <strong>Mann-Whitney U + Cohen's d</strong> for continuous features. These tell us which features are worth putting in the model.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV7fmAOulv0Q"
      },
      "source": [
        "### Cram√©r's V ‚Äî Measuring Association Between Categories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdEG9NqXlv0Q"
      },
      "source": [
        "# Helper function: Cram√©r's V\n",
        "def cramers_v(x, y):\n",
        "    \"\"\"Calculate Cram√©r's V between two categorical Series.\"\"\"\n",
        "    ct = pd.crosstab(x, y)\n",
        "    chi2 = chi2_contingency(ct)[0]\n",
        "    n = ct.sum().sum()\n",
        "    r, k = ct.shape\n",
        "    return np.sqrt(chi2 / (n * (min(r, k) - 1)))\n",
        "\n",
        "# Identify categorical columns (object type + SeniorCitizen which is 0/1)\n",
        "cat_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "cat_cols = [c for c in cat_cols if c != \"Churn\"]\n",
        "\n",
        "# Compute Cram√©r's V for each categorical feature vs Churn\n",
        "cramers_results = pd.DataFrame({\n",
        "    \"Feature\": cat_cols,\n",
        "    \"Cram√©r's V\": [cramers_v(df[col], df[\"Churn\"]) for col in cat_cols]\n",
        "}).sort_values(\"Cram√©r's V\", ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.barh(cramers_results[\"Feature\"], cramers_results[\"Cram√©r's V\"], color=\"steelblue\")\n",
        "plt.xlabel(\"Cram√©r's V (0 = no association, 1 = perfect association)\")\n",
        "plt.title(\"Categorical Features vs Churn ‚Äî Cram√©r's V\")\n",
        "plt.axvline(x=0.1, color=\"orange\", linestyle=\"--\", alpha=0.7, label=\"Weak threshold (0.1)\")\n",
        "plt.axvline(x=0.3, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Moderate threshold (0.3)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Top 5 categorical churn drivers:\")\n",
        "print(cramers_results.head(5).to_string(index=False))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Y8LheFLlv0Q"
      },
      "source": [
        "<div style=\"background-color: #D6EAF8; border-left: 5px solid #2E86C1; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1A5276;\">üí° READING CRAM√âR'S V</strong><br>\n",
        "  <ul>\n",
        "    <li><strong>< 0.1:</strong> Negligible ‚Äî this feature probably doesn't help predict churn</li>\n",
        "    <li><strong>0.1‚Äì0.3:</strong> Weak to moderate ‚Äî worth including in the model</li>\n",
        "    <li><strong>> 0.3:</strong> Strong ‚Äî this feature is a major churn driver</li>\n",
        "  </ul>\n",
        "  <code>Contract</code> and <code>InternetService</code> should be near the top. Month-to-month contracts and fiber optic internet are the biggest churn signals.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvbOItPGlv0Q"
      },
      "source": [
        "### Mann-Whitney U + Cohen's d ‚Äî Continuous Features vs Churn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7siCDZgNlv0Q"
      },
      "source": [
        "# Helper: Cohen's d\n",
        "def cohens_d(group1, group2):\n",
        "    \"\"\"Effect size: how far apart are the two groups in standard deviations?\"\"\"\n",
        "    n1, n2 = len(group1), len(group2)\n",
        "    pooled_std = np.sqrt(((n1 - 1) * group1.std()**2 + (n2 - 1) * group2.std()**2) / (n1 + n2 - 2))\n",
        "    return (group1.mean() - group2.mean()) / pooled_std if pooled_std > 0 else 0\n",
        "\n",
        "# Continuous features\n",
        "num_cols = [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"]\n",
        "churn_yes = df[df[\"Churn\"] == \"Yes\"]\n",
        "churn_no = df[df[\"Churn\"] == \"No\"]\n",
        "\n",
        "mw_results = []\n",
        "for col in num_cols:\n",
        "    u_stat, p_val = mannwhitneyu(churn_yes[col], churn_no[col], alternative=\"two-sided\")\n",
        "    d = cohens_d(churn_yes[col], churn_no[col])\n",
        "    mw_results.append({\"Feature\": col, \"U Statistic\": f\"{u_stat:,.0f}\",\n",
        "                        \"p-value\": f\"{p_val:.2e}\", \"Cohen's d\": f\"{d:.3f}\",\n",
        "                        \"Effect\": \"Large\" if abs(d) > 0.8 else \"Medium\" if abs(d) > 0.5 else \"Small\"})\n",
        "\n",
        "mw_df = pd.DataFrame(mw_results)\n",
        "print(\"Mann-Whitney U Test + Cohen's d (Churned vs Stayed):\")\n",
        "print(mw_df.to_string(index=False))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBm0OzLDlv0R"
      },
      "source": [
        "# Visualize: distribution of continuous features by churn status\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "for ax, col in zip(axes, num_cols):\n",
        "    for label, color in [(\"No\", \"steelblue\"), (\"Yes\", \"salmon\")]:\n",
        "        subset = df[df[\"Churn\"] == label][col]\n",
        "        ax.hist(subset, bins=30, alpha=0.6, color=color, label=f\"Churn={label}\")\n",
        "    ax.set_title(f\"{col} by Churn Status\")\n",
        "    ax.set_xlabel(col)\n",
        "    ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFjyo7A3lv0R"
      },
      "source": [
        "# Quick check: point-biserial correlation\n",
        "churn_binary = (df[\"Churn\"] == \"Yes\").astype(int)\n",
        "print(\"Point-Biserial Correlation with Churn:\")\n",
        "for col in num_cols:\n",
        "    r, p = pointbiserialr(churn_binary, df[col])\n",
        "    print(f\"  {col:20s} r = {r:+.3f}  (p = {p:.2e})\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzMqSaTQlv0R"
      },
      "source": [
        "<div style=\"background-color: #FADBD8; border-left: 5px solid #E74C3C; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #922B21;\">üõë STOP AND CHECK ‚Äî Checkpoint 1</strong><br>\n",
        "  <ul>\n",
        "    <li><strong>Cram√©r's V:</strong> Contract and InternetService should be the top 2 categorical drivers</li>\n",
        "    <li><strong>Cohen's d:</strong> tenure should show a large negative effect (churners have shorter tenure)</li>\n",
        "    <li><strong>Point-biserial:</strong> tenure should have a negative correlation with churn</li>\n",
        "  </ul>\n",
        "  The story is forming: <em>new customers on month-to-month contracts with fiber optic internet are the highest risk.</em>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hbl1yhZzlv0R"
      },
      "source": [
        "---\n",
        "# Example 2 ‚Äî Logistic Regression Baseline + Business Cost of Churn\n",
        "\n",
        "<div style=\"background-color: #D6EAF8; border-left: 5px solid #2E86C1; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1A5276;\">üí° WHY ARE WE DOING THIS?</strong><br>\n",
        "  A model is only useful if leadership understands <strong>what it costs to get it wrong</strong>. Before building anything, we put a dollar figure on churn. Then we build logistic regression ‚Äî the interpretable baseline that can tell leadership <em>why</em> customers are leaving, not just <em>which</em> ones.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-XLmLaLlv0R"
      },
      "source": [
        "# Business cost of churn\n",
        "avg_monthly = df[\"MonthlyCharges\"].mean()\n",
        "avg_tenure = df[df[\"Churn\"] == \"No\"][\"tenure\"].mean()\n",
        "churned_count = (df[\"Churn\"] == \"Yes\").sum()\n",
        "acquisition_cost = 300  # Industry benchmark for telecom\n",
        "\n",
        "# Annual revenue lost from churned customers\n",
        "annual_revenue_lost = churned_count * avg_monthly * 12\n",
        "# Lifetime value lost (remaining months they would have stayed)\n",
        "avg_remaining = avg_tenure - df[df[\"Churn\"] == \"Yes\"][\"tenure\"].mean()\n",
        "lifetime_lost = churned_count * avg_monthly * avg_remaining\n",
        "# Replacement cost\n",
        "replacement_cost = churned_count * acquisition_cost\n",
        "\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"  BUSINESS COST OF CHURN ‚Äî Telco Dataset\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"  Churned customers:        {churned_count:,}\")\n",
        "print(f\"  Avg monthly charge:       ${avg_monthly:,.2f}\")\n",
        "print(f\"  Avg tenure (stayed):      {avg_tenure:.0f} months\")\n",
        "print(f\"  Acquisition cost/customer: ${acquisition_cost}\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"  Annual revenue at risk:   ${annual_revenue_lost:,.0f}\")\n",
        "print(f\"  Lifetime value lost:      ${lifetime_lost:,.0f}\")\n",
        "print(f\"  Replacement cost:         ${replacement_cost:,.0f}\")\n",
        "print(f\"  TOTAL ESTIMATED IMPACT:   ${lifetime_lost + replacement_cost:,.0f}\")\n",
        "print(f\"{'='*50}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSwiUkiLlv0R"
      },
      "source": [
        "### Preprocessing Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jskbapsnlv0R"
      },
      "source": [
        "# Full preprocessing pipeline\n",
        "df_model = df.copy()\n",
        "\n",
        "# Simplify \"No internet service\" / \"No phone service\" ‚Üí \"No\"\n",
        "replace_cols = [\"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\",\n",
        "                \"TechSupport\", \"StreamingTV\", \"StreamingMovies\", \"MultipleLines\"]\n",
        "for col in replace_cols:\n",
        "    df_model[col] = df_model[col].replace({\"No internet service\": \"No\", \"No phone service\": \"No\"})\n",
        "\n",
        "# Binary encode Yes/No columns\n",
        "binary_cols = [\"Partner\", \"Dependents\", \"PhoneService\", \"PaperlessBilling\", \"Churn\"]\n",
        "for col in binary_cols:\n",
        "    df_model[col] = df_model[col].map({\"Yes\": 1, \"No\": 0})\n",
        "\n",
        "# Encode gender\n",
        "df_model[\"gender\"] = df_model[\"gender\"].map({\"Male\": 1, \"Female\": 0})\n",
        "\n",
        "# Encode remaining binary Yes/No columns\n",
        "for col in replace_cols:\n",
        "    df_model[col] = df_model[col].map({\"Yes\": 1, \"No\": 0})\n",
        "\n",
        "# One-hot encode multi-category columns\n",
        "df_model = pd.get_dummies(df_model, columns=[\"InternetService\", \"Contract\", \"PaymentMethod\"],\n",
        "                           drop_first=True, dtype=int)\n",
        "\n",
        "# Separate features and target\n",
        "X = df_model.drop(columns=[\"Churn\"])\n",
        "y = df_model[\"Churn\"]\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale continuous features\n",
        "scaler = StandardScaler()\n",
        "continuous = [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"]\n",
        "X_train[continuous] = scaler.fit_transform(X_train[continuous])\n",
        "X_test[continuous] = scaler.transform(X_test[continuous])\n",
        "\n",
        "feature_names = X_train.columns.tolist()\n",
        "n_features = len(feature_names)\n",
        "\n",
        "print(f\"Features: {n_features}\")\n",
        "print(f\"Train: {X_train.shape[0]:,} | Test: {X_test.shape[0]:,}\")\n",
        "print(f\"Churn rate ‚Äî Train: {y_train.mean():.1%} | Test: {y_test.mean():.1%}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNXOTNV6lv0S"
      },
      "source": [
        "### Logistic Regression Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aW2DOtQlv0S"
      },
      "source": [
        "# Logistic Regression\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "lr_predictions = lr_model.predict(X_test)\n",
        "lr_probabilities = lr_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Logistic Regression ‚Äî Classification Report:\")\n",
        "print(classification_report(y_test, lr_predictions, target_names=[\"Stayed\", \"Churned\"]))\n",
        "\n",
        "lr_accuracy = accuracy_score(y_test, lr_predictions)\n",
        "lr_auc = roc_auc_score(y_test, lr_probabilities)\n",
        "print(f\"Accuracy: {lr_accuracy:.4f}\")\n",
        "print(f\"AUC:      {lr_auc:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCLlUfSklv0S"
      },
      "source": [
        "# Confusion matrix\n",
        "cm_lr = confusion_matrix(y_test, lr_predictions)\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "sns.heatmap(cm_lr, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Stayed\", \"Churned\"],\n",
        "            yticklabels=[\"Stayed\", \"Churned\"], ax=ax)\n",
        "ax.set_xlabel(\"Predicted\")\n",
        "ax.set_ylabel(\"Actual\")\n",
        "ax.set_title(\"Confusion Matrix ‚Äî Logistic Regression\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "tn, fp, fn, tp = cm_lr.ravel()\n",
        "print(f\"True Negatives  (correctly predicted stayed):  {tn}\")\n",
        "print(f\"False Positives (predicted churn, actually stayed): {fp}\")\n",
        "print(f\"False Negatives (predicted stayed, actually churned): {fn}  ‚Üê COSTLY\")\n",
        "print(f\"True Positives  (correctly predicted churned):  {tp}\")\n",
        "print(f\"\\nüí∞ Each False Negative = a churner we MISSED ‚Äî they leave without intervention\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9t8v_GRlv0S"
      },
      "source": [
        "# Top churn drivers ‚Äî coefficient interpretation\n",
        "coef_df = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Coefficient\": lr_model.coef_[0]\n",
        "}).sort_values(\"Coefficient\", ascending=False)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "top_pos = coef_df.head(5)\n",
        "top_neg = coef_df.tail(5)\n",
        "display_df = pd.concat([top_pos, top_neg])\n",
        "\n",
        "colors = [\"salmon\" if c > 0 else \"steelblue\" for c in display_df[\"Coefficient\"]]\n",
        "ax.barh(display_df[\"Feature\"], display_df[\"Coefficient\"], color=colors)\n",
        "ax.set_xlabel(\"Coefficient (positive = increases churn probability)\")\n",
        "ax.set_title(\"Top 5 Positive & Negative Churn Drivers\")\n",
        "ax.axvline(x=0, color=\"black\", linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Top 5 features INCREASING churn risk:\")\n",
        "for _, row in top_pos.iterrows():\n",
        "    print(f\"  {row['Feature']:45s} {row['Coefficient']:+.4f}\")\n",
        "print(\"\\nTop 5 features DECREASING churn risk:\")\n",
        "for _, row in top_neg.iterrows():\n",
        "    print(f\"  {row['Feature']:45s} {row['Coefficient']:+.4f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2ayQAjblv0S"
      },
      "source": [
        "<div style=\"background-color: #D6EAF8; border-left: 5px solid #2E86C1; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1A5276;\">üí° READING THE COEFFICIENTS</strong><br>\n",
        "  <ul>\n",
        "    <li><strong>Month-to-month contract</strong> (positive, large): Biggest churn driver. No commitment = easy to leave.</li>\n",
        "    <li><strong>Fiber optic internet</strong> (positive): Higher churn than DSL ‚Äî possibly price sensitivity or service issues.</li>\n",
        "    <li><strong>Electronic check payment</strong> (positive): Less \"sticky\" than auto-pay ‚Äî no friction to stop paying.</li>\n",
        "    <li><strong>Tenure</strong> (negative, large): Longer tenure = less likely to churn. Loyalty builds over time.</li>\n",
        "    <li><strong>Two-year contract</strong> (negative): Lock-in reduces churn. The business implication is clear: <em>get customers onto contracts.</em></li>\n",
        "  </ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_U6Rjbslv0S"
      },
      "source": [
        "# Risk scoring ‚Äî rank customers by churn probability\n",
        "risk_df = X_test.copy()\n",
        "risk_df[\"churn_probability\"] = lr_probabilities\n",
        "risk_df[\"actual_churn\"] = y_test.values\n",
        "risk_df = risk_df.sort_values(\"churn_probability\", ascending=False)\n",
        "\n",
        "high_risk = risk_df[risk_df[\"churn_probability\"] >= 0.5]\n",
        "print(f\"High-risk customers (prob ‚â• 0.5): {len(high_risk):,}\")\n",
        "print(f\"Of those, actually churned: {high_risk['actual_churn'].sum():,} ({high_risk['actual_churn'].mean():.1%})\")\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.hist(lr_probabilities[y_test == 0], bins=30, alpha=0.6, color=\"steelblue\", label=\"Stayed\")\n",
        "plt.hist(lr_probabilities[y_test == 1], bins=30, alpha=0.6, color=\"salmon\", label=\"Churned\")\n",
        "plt.xlabel(\"Predicted Churn Probability\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Distribution of Churn Probabilities by Actual Outcome\")\n",
        "plt.legend()\n",
        "plt.axvline(x=0.5, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Threshold (0.5)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-dBAHgGlv0S"
      },
      "source": [
        "<div style=\"background-color: #FADBD8; border-left: 5px solid #E74C3C; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #922B21;\">üõë STOP AND CHECK ‚Äî Checkpoint 2 (End of Session 1)</strong><br>\n",
        "  <ul>\n",
        "    <li>Logistic regression accuracy ‚âà 80%, recall on churners ‚âà 54%</li>\n",
        "    <li>AUC ‚âà 0.84 ‚Äî good but not great</li>\n",
        "    <li>The model catches about half of actual churners ‚Äî the other half slip through</li>\n",
        "    <li>Coefficients tell a clear story: month-to-month + fiber optic + electronic check = highest risk</li>\n",
        "  </ul>\n",
        "  <strong>Session 1 ends here. Session 2 picks up with neural networks.</strong>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Bwe9Lv0lv0T"
      },
      "source": [
        "---\n",
        "# SESSION 2 ‚Äî Chapter 5: Neural Networks\n",
        "\n",
        "---\n",
        "# Example 3 ‚Äî Single-Neuron Neural Network (The Bridge)\n",
        "\n",
        "<div style=\"background-color: #D6EAF8; border-left: 5px solid #2E86C1; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1A5276;\">üí° WHY ARE WE DOING THIS?</strong><br>\n",
        "  Before adding layers and complexity, we prove something powerful: a neural network with <strong>one neuron and sigmoid activation</strong> is mathematically identical to logistic regression. Same inputs, same activation function, same output. The results should be nearly identical ‚Äî and that's the point.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2J_1BuFlv0T"
      },
      "source": [
        "# Single-neuron neural network = logistic regression\n",
        "model_single = Sequential([\n",
        "    Dense(1, activation=\"sigmoid\", input_shape=(n_features,))\n",
        "])\n",
        "\n",
        "model_single.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model_single.summary()\n",
        "print(f\"\\nTotal parameters: {n_features + 1} (one weight per feature + 1 bias)\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA0wvkqglv0T"
      },
      "source": [
        "# Train the single neuron\n",
        "history_single = model_single.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100, batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "single_loss, single_acc = model_single.evaluate(X_test, y_test, verbose=0)\n",
        "single_pred = (model_single.predict(X_test, verbose=0) > 0.5).astype(int).ravel()\n",
        "\n",
        "print(f\"\\nSingle-Neuron ANN:\")\n",
        "print(f\"  Accuracy: {single_acc:.4f}\")\n",
        "print(f\"  LR Accuracy: {lr_accuracy:.4f}\")\n",
        "print(f\"  Difference: {abs(single_acc - lr_accuracy):.4f}\")\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, single_pred, target_names=[\"Stayed\", \"Churned\"]))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hozYh0molv0T"
      },
      "source": [
        "<div style=\"background-color: #D5F5E3; border-left: 5px solid #27AE60; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1E8449;\">‚úÖ TEACHING MOMENT</strong><br>\n",
        "  The numbers are nearly identical. A neural network with one neuron <strong>IS</strong> logistic regression. The power of neural networks comes from <strong>adding hidden layers</strong> ‚Äî that's what lets them learn nonlinear patterns that logistic regression cannot.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUGa_UhXlv0T"
      },
      "source": [
        "---\n",
        "# Example 4 ‚Äî Three-Layer ANN (Overfitting Demo)\n",
        "\n",
        "<div style=\"background-color: #D6EAF8; border-left: 5px solid #2E86C1; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1A5276;\">üí° WHY ARE WE DOING THIS?</strong><br>\n",
        "  More neurons = more power, but also more risk of <strong>overfitting</strong> (memorizing the training data instead of learning patterns). This example deliberately shows what overfitting looks like in the loss curves so you can diagnose it in your own models.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADEt7b7nlv0T"
      },
      "source": [
        "# Three-layer ANN ‚Äî no regularization\n",
        "model_overfit = Sequential([\n",
        "    Dense(n_features, activation=\"relu\", input_shape=(n_features,)),\n",
        "    Dense(15, activation=\"relu\"),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model_overfit.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model_overfit.summary()\n",
        "print(f\"\\nParameters: {model_overfit.count_params():,} (vs {n_features + 1} in single neuron)\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8nrUyCOlv0T"
      },
      "source": [
        "# Train WITHOUT regularization ‚Äî watch for overfitting\n",
        "history_overfit = model_overfit.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100, batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print(f\"Training complete: 100 epochs (no early stopping)\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT2XUai9lv0U"
      },
      "source": [
        "# Plot loss curves ‚Äî diagnose overfitting\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Loss\n",
        "axes[0].plot(history_overfit.history[\"loss\"], label=\"Training Loss\", color=\"steelblue\")\n",
        "axes[0].plot(history_overfit.history[\"val_loss\"], label=\"Validation Loss\", color=\"salmon\")\n",
        "axes[0].set_title(\"Loss Curves ‚Äî 3-Layer ANN (No Regularization)\")\n",
        "axes[0].set_xlabel(\"Epoch\")\n",
        "axes[0].set_ylabel(\"Loss\")\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy\n",
        "axes[1].plot(history_overfit.history[\"accuracy\"], label=\"Training Accuracy\", color=\"steelblue\")\n",
        "axes[1].plot(history_overfit.history[\"val_accuracy\"], label=\"Validation Accuracy\", color=\"salmon\")\n",
        "axes[1].set_title(\"Accuracy Curves ‚Äî 3-Layer ANN (No Regularization)\")\n",
        "axes[1].set_xlabel(\"Epoch\")\n",
        "axes[1].set_ylabel(\"Accuracy\")\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11hSiIv9lv0U"
      },
      "source": [
        "# Evaluate ‚Äî likely WORSE than logistic regression\n",
        "overfit_loss, overfit_acc = model_overfit.evaluate(X_test, y_test, verbose=0)\n",
        "overfit_pred = (model_overfit.predict(X_test, verbose=0) > 0.5).astype(int).ravel()\n",
        "\n",
        "print(f\"3-Layer ANN (no regularization):\")\n",
        "print(f\"  Accuracy: {overfit_acc:.4f} (LR was {lr_accuracy:.4f})\")\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, overfit_pred, target_names=[\"Stayed\", \"Churned\"]))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5LhVlzZlv0U"
      },
      "source": [
        "<div style=\"background-color: #FEF9E7; border-left: 5px solid #F1C40F; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #7D6608;\">‚ö†Ô∏è THE OVERFITTING DIAGNOSIS</strong><br>\n",
        "  Look at the loss curves: training loss drops smoothly, but <strong>validation loss rises after ~30 epochs</strong>. The model is memorizing the training data instead of learning generalizable patterns. More parameters didn't help ‚Äî they made things worse. We need two tools: <strong>Dropout</strong> (randomly disable neurons during training) and <strong>Early Stopping</strong> (stop training when validation loss starts rising).\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXRz7APOlv0U"
      },
      "source": [
        "---\n",
        "# Example 5 ‚Äî Full Pipeline: Tuned ANN + Model Comparison\n",
        "\n",
        "<div style=\"background-color: #D6EAF8; border-left: 5px solid #2E86C1; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1A5276;\">üí° WHY ARE WE DOING THIS?</strong><br>\n",
        "  This is the closing case study. We fix the overfitting with <strong>Dropout + Early Stopping</strong>, then compare the tuned ANN against logistic regression head-to-head. The first half runs pre-filled. The second half is your turn ‚Äî you'll evaluate the model and build the comparison.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojl6_ALqlv0U"
      },
      "source": [
        "# Tuned ANN ‚Äî Dropout + Early Stopping\n",
        "model_tuned = Sequential([\n",
        "    Dense(n_features, activation=\"relu\", input_shape=(n_features,)),\n",
        "    Dropout(0.3),\n",
        "    Dense(15, activation=\"relu\"),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model_tuned.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model_tuned.summary()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WLwfG-4lv0Z"
      },
      "source": [
        "# Early Stopping callback\n",
        "early_stop = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train with early stopping\n",
        "history_tuned = model_tuned.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=200,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "actual_epochs = len(history_tuned.history[\"loss\"])\n",
        "print(f\"\\n‚úÖ Training stopped at epoch {actual_epochs} (max was 200)\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv3jw6yjlv0a"
      },
      "source": [
        "# Improved loss curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].plot(history_tuned.history[\"loss\"], label=\"Training Loss\", color=\"steelblue\")\n",
        "axes[0].plot(history_tuned.history[\"val_loss\"], label=\"Validation Loss\", color=\"salmon\")\n",
        "axes[0].axvline(x=actual_epochs - 1, color=\"green\", linestyle=\"--\", alpha=0.5, label=f\"Early Stop (epoch {actual_epochs})\")\n",
        "axes[0].set_title(\"Loss Curves ‚Äî Tuned ANN (Dropout + Early Stopping)\")\n",
        "axes[0].set_xlabel(\"Epoch\")\n",
        "axes[0].set_ylabel(\"Loss\")\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(history_tuned.history[\"accuracy\"], label=\"Training Accuracy\", color=\"steelblue\")\n",
        "axes[1].plot(history_tuned.history[\"val_accuracy\"], label=\"Validation Accuracy\", color=\"salmon\")\n",
        "axes[1].axvline(x=actual_epochs - 1, color=\"green\", linestyle=\"--\", alpha=0.5, label=f\"Early Stop (epoch {actual_epochs})\")\n",
        "axes[1].set_title(\"Accuracy Curves ‚Äî Tuned ANN (Dropout + Early Stopping)\")\n",
        "axes[1].set_xlabel(\"Epoch\")\n",
        "axes[1].set_ylabel(\"Accuracy\")\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6gJp0fmlv0a"
      },
      "source": [
        "<div style=\"background-color: #FADBD8; border-left: 5px solid #E74C3C; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #922B21;\">üõë STOP AND CHECK ‚Äî Checkpoint 3</strong><br>\n",
        "  <ul>\n",
        "    <li>Training should stop between epochs 30‚Äì60</li>\n",
        "    <li>The gap between training and validation loss should be much smaller than Example 4</li>\n",
        "    <li>If it ran all 200 epochs, EarlyStopping isn't configured correctly</li>\n",
        "  </ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33Pq8VI7lv0a"
      },
      "source": [
        "---\n",
        "## Your Turn ‚Äî Evaluate and Compare\n",
        "\n",
        "<div style=\"background-color: #D5F5E3; border-left: 5px solid #27AE60; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1E8449;\">‚úÖ DO THIS ‚Äî Live Class Participation</strong><br>\n",
        "  Complete the cells below to evaluate the tuned ANN and compare it to logistic regression.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpMwGe0Tlv0a"
      },
      "source": [
        "# YOUR CODE HERE ‚Äî Evaluate the tuned ANN\n",
        "# 1. Generate predictions (threshold 0.5) ‚Üí store in: ann_predictions\n",
        "# 2. Generate probabilities ‚Üí store in: ann_probabilities\n",
        "# 3. Print the classification report\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Wavi_zmlv0a"
      },
      "source": [
        "# YOUR CODE HERE ‚Äî ROC Curve Comparison\n",
        "# 1. Calculate ROC curve for LR (lr_probabilities already exists)\n",
        "# 2. Calculate ROC curve for ANN (ann_probabilities from above)\n",
        "# 3. Plot BOTH on a single figure\n",
        "# 4. Include AUC in the legend\n",
        "# Colors: LR = \"#0f3460\" (navy), ANN = \"#e94560\" (coral)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnaI3mdJlv0b"
      },
      "source": [
        "# YOUR CODE HERE ‚Äî Side-by-side comparison table\n",
        "# Build a DataFrame comparing:\n",
        "#   Accuracy, Precision (Churned), Recall (Churned), F1 (Churned), AUC\n",
        "# for both Logistic Regression and Tuned ANN\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnyB3Im6lv0b"
      },
      "source": [
        "# YOUR CODE HERE ‚Äî Customers flagged by ANN but missed by LR\n",
        "# Find customers where ANN predicted churn but LR did not\n",
        "# How many additional customers does the ANN catch?\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwsY0rzMlv0b"
      },
      "source": [
        "---\n",
        "## Takeaway\n",
        "\n",
        "<div style=\"background-color: #D5F5E3; border-left: 5px solid #27AE60; padding: 15px; margin: 15px 0; border-radius: 4px;\">\n",
        "  <strong style=\"color: #1E8449;\">‚úÖ WHAT WE BUILT TODAY</strong><br>\n",
        "  A complete churn prediction pipeline ‚Äî from raw data through statistical analysis, logistic regression, and neural networks. The logistic regression gave us an interpretable baseline; the neural network found additional patterns at the cost of explainability. In practice, many teams deploy both.\n",
        "</div>\n",
        "\n",
        "**Regression vs Classification comparison:**\n",
        "\n",
        "| | Week 3 (Regression) | Week 4 (Classification) |\n",
        "|---|---|---|\n",
        "| Target | Continuous (dollars) | Binary (churn yes/no) |\n",
        "| EDA tools | Correlation, scatterplots | Cram√©r's V, Mann-Whitney U, Cohen's d |\n",
        "| Baseline model | Linear Regression | Logistic Regression |\n",
        "| Advanced model | Multiple Regression | Neural Network |\n",
        "| Evaluation | R¬≤, RMSE | Accuracy, Precision, Recall, F1, AUC |\n",
        "\n",
        "**Next chapter preview:** We shift from predicting *individual* outcomes to discovering *population-level* patterns ‚Äî from supervised to unsupervised learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8yjzy8dlv0b"
      },
      "source": [
        "---\n",
        "<p style=\"color:#7F8C8D; font-size:0.85em;\">\n",
        "<em>CAP4767 Data Mining with Python | Miami Dade College | Spring 2026</em><br>\n",
        "Week 4 Demo ‚Äî Customer Churn: EDA ‚Üí Logistic Regression ‚Üí Neural Networks\n",
        "</p>"
      ]
    }
  ]
}